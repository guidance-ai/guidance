{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b6b7d0-dd86-4275-be30-c83df4b925ea",
   "metadata": {},
   "source": [
    "# Direct Engine Access\n",
    "\n",
    "This notebook is about playing around to create an entry point where the user manages their state themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcd8597-9429-4cd3-8259-0c82139fb210",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Please install llama-cpp-python with `pip install llama-cpp-python` in order to use guidance.models.LlamaCpp!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguidance\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Define the model we will use\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m lm = \u001b[43mguidance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/path/to/model.gguf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# lm = guidance.models.Transformers(\"microsoft/Phi-3-mini-4k-instruct\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\repos\\guidance\\guidance\\models\\_llama_cpp.py:253\u001b[39m, in \u001b[36mLlamaCpp.__init__\u001b[39m\u001b[34m(self, model, echo, chat_template, enable_backtrack, enable_ff_tokens, enable_monitoring, sampling_params, **llama_cpp_kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    241\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    242\u001b[39m     model=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m     **llama_cpp_kwargs,\n\u001b[32m    250\u001b[39m ):\n\u001b[32m    251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a new LlamaCpp model object that represents a model in a given state.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     engine = \u001b[43mLlamaCppEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_backtrack\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_backtrack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_ff_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_ff_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_monitoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_monitoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_token_probabilities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_top_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mllama_cpp_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     interpreter = EngineInterpreter(engine)\n\u001b[32m    264\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    265\u001b[39m         interpreter=interpreter,\n\u001b[32m    266\u001b[39m         sampling_params=SamplingParams() \u001b[38;5;28;01mif\u001b[39;00m sampling_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sampling_params,\n\u001b[32m    267\u001b[39m         echo=echo,\n\u001b[32m    268\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\repos\\guidance\\guidance\\models\\_llama_cpp.py:94\u001b[39m, in \u001b[36mLlamaCppEngine.__init__\u001b[39m\u001b[34m(self, model, chat_template, enable_backtrack, enable_ff_tokens, enable_monitoring, enable_token_probabilities, enable_top_k, top_k, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     83\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     **kwargs,\n\u001b[32m     92\u001b[39m ):\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_llama_cpp:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m     95\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease install llama-cpp-python with `pip install llama-cpp-python` in order to use guidance.models.LlamaCpp!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         )\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Path):\n\u001b[32m     99\u001b[39m         model = \u001b[38;5;28mstr\u001b[39m(model)\n",
      "\u001b[31mException\u001b[39m: Please install llama-cpp-python with `pip install llama-cpp-python` in order to use guidance.models.LlamaCpp!"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "\n",
    "# Define the model we will use\n",
    "lm = guidance.models.LlamaCpp(\"/path/to/model.gguf\", n_gpu_layers=-1)\n",
    "# lm = guidance.models.Transformers(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a9cbc-ae9d-4727-8c31-28ff7e7daee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\": \"What is your name?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799135a0-5221-4182-91e0-98d335bb74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"%llguidance {}\n",
    "\n",
    "start: START\n",
    "START: \"Aa\"{2,} \"Bb\"{1,3}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2249736-7a9f-4de9-8a84-fac271e37e4b",
   "metadata": {},
   "source": [
    "Grab the `engine` object out of the Model. Note that this is only going to work for local models (basically, Models contain Interpreters, but only the local Interpreters then contain an engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ef1fe-eb9f-4412-9941-d74de7c76301",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = lm._interpreter.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9945d-6dcc-4f18-8440-4308cb3011cd",
   "metadata": {},
   "source": [
    "There should be a chat template available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658a71c-66a6-4d59-89a1-70fe63e0f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = engine.get_chat_template().template_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5631ea-a909-4ba1-a4f3-eb845c99d9f7",
   "metadata": {},
   "source": [
    "Render the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8504e7-9f40-4727-a0fe-d94f3241de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, BaseLoader\n",
    "\n",
    "rtemplate = Environment(loader=BaseLoader).from_string(chat_template)\n",
    "rendered_prompt = rtemplate.render(messages=messages, eos_token=engine.tokenizer.eos_token.decode(\"utf-8\"))\n",
    "rendered_prompt += engine.get_chat_template().get_role_start(\"assistant\")\n",
    "\n",
    "print(\"Rendered Prompt:\\n\", rendered_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8dc6c-c6c7-48bf-9819-dd9c54bad1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = guidance.models._engine.EngineState()\n",
    "\n",
    "state.prompt = rendered_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1e1d5-0c67-4868-bec9-9655c32ee43e",
   "metadata": {},
   "source": [
    "Run the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe89ca-84b0-4382-9a1d-931a246900e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = bytearray()\n",
    "for nxt in engine(state, grammar):\n",
    "    nxt_tokens = [x.token_id for x in nxt.tokens]\n",
    "    nxt_bytes = engine.tokenizer.decode(nxt_tokens)\n",
    "    full_response += nxt_bytes\n",
    "print(full_response.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d698c-47a4-48e7-ae36-db4bf4e8cdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
