{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f8afbd-f8bc-4336-a354-8de10da587fe",
   "metadata": {},
   "source": [
    "# Benchmarking Structured JSON Output\n",
    "\n",
    "Structuring language models' (LM) output into machine readable form is one of the core use cases for guidance. When an LM is part of a data processing pipeline, getting the output structure right 100% of the time is paramount to reducing late night on-calls for the practitioner responsible.\n",
    "\n",
    "We will be benchmarking structured JSON output utilizing LangChain's chat extraction dataset. LangChain has done good work providing an industry-relevant task. The respective blog post can be found [here](https://blog.langchain.dev/extraction-benchmarking/). For our purposes, we are most interested in the output JSON expected from this.\n",
    "\n",
    "## The JSON Schema\n",
    "\n",
    "The expected JSON output is complex yet not unreasonable for a pipeline: nested structures, conditional fields and constraints on some values. To date, LMs vary in accuracy at producing JSON outputs, regardless of schema validity.\n",
    " \n",
    "```json\n",
    "{\n",
    "  \"title\": \"GenerateTicket\",\n",
    "  \"description\": \"Generate a ticket containing all the extracted information.\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"issue_summary\": {\n",
    "      \"title\": \"Issue Summary\",\n",
    "      \"description\": \"short (<10 word) summary of the issue or question\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"question\": {\n",
    "      \"title\": \"Question\",\n",
    "      \"description\": \"Information inferred from the the question.\",\n",
    "      \"allOf\": [\n",
    "        {\n",
    "          \"$ref\": \"#/definitions/QuestionCategorization\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"response\": {\n",
    "      \"title\": \"Response\",\n",
    "      \"description\": \"Information inferred from the the response.\",\n",
    "      \"allOf\": [\n",
    "        {\n",
    "          \"$ref\": \"#/definitions/ResponseCategorization\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\n",
    "    \"issue_summary\",\n",
    "    \"question\",\n",
    "    \"response\"\n",
    "  ],\n",
    "  \"definitions\": {\n",
    "    \"QuestionCategory\": {\n",
    "      \"title\": \"QuestionCategory\",\n",
    "      \"description\": \"An enumeration.\",\n",
    "      \"enum\": [\n",
    "        \"Implementation Issues\",\n",
    "        \"Feature Requests\",\n",
    "        \"Concept Explanations\",\n",
    "        \"Code Optimization\",\n",
    "        \"Security and Privacy Concerns\",\n",
    "        \"Model Training and Fine-tuning\",\n",
    "        \"Data Handling and Manipulation\",\n",
    "        \"User Interaction Flow\",\n",
    "        \"Technical Integration\",\n",
    "        \"Error Handling and Logging\",\n",
    "        \"Customization and Configuration\",\n",
    "        \"External API and Data Source Integration\",\n",
    "        \"Language and Localization\",\n",
    "        \"Streaming and Real-time Processing\",\n",
    "        \"Tool Development\",\n",
    "        \"Function Calling\",\n",
    "        \"LLM Integrations\",\n",
    "        \"General Agent Question\",\n",
    "        \"General Chit Chat\",\n",
    "        \"Memory\",\n",
    "        \"Debugging Help\",\n",
    "        \"Application Design\",\n",
    "        \"Prompt Templates\",\n",
    "        \"Cost Tracking\",\n",
    "        \"Other\"\n",
    "      ],\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"Sentiment\": {\n",
    "      \"title\": \"Sentiment\",\n",
    "      \"description\": \"An enumeration.\",\n",
    "      \"enum\": [\n",
    "        \"Negative\",\n",
    "        \"Neutral\",\n",
    "        \"Positive\"\n",
    "      ],\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"ProgrammingLanguage\": {\n",
    "      \"title\": \"ProgrammingLanguage\",\n",
    "      \"description\": \"An enumeration.\",\n",
    "      \"enum\": [\n",
    "        \"python\",\n",
    "        \"javascript\",\n",
    "        \"typescript\",\n",
    "        \"unknown\",\n",
    "        \"other\"\n",
    "      ],\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"QuestionCategorization\": {\n",
    "      \"title\": \"QuestionCategorization\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"question_category\": {\n",
    "          \"$ref\": \"#/definitions/QuestionCategory\"\n",
    "        },\n",
    "        \"category_if_other\": {\n",
    "          \"title\": \"Category If Other\",\n",
    "          \"description\": \"question category if the category above is 'other'\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"is_off_topic\": {\n",
    "          \"title\": \"Is Off Topic\",\n",
    "          \"description\": \"If the input is general chit chat or does not pertain to technical inqueries about LangChain or building/debugging applications with LLMs/AI, it is off topic. For context, LangChain is a library and framework designed to assist in building applications with LLMs. Questions may also be about similar packages like LangServe, LangSmith, OpenAI, Anthropic, vectorstores, agents, etc.\",\n",
    "          \"type\": \"boolean\"\n",
    "        },\n",
    "        \"toxicity\": {\n",
    "          \"title\": \"Toxicity\",\n",
    "          \"description\": \"Whether or not the input question is toxic\",\n",
    "          \"exclusiveMaximum\": 6,\n",
    "          \"minimum\": 0,\n",
    "          \"type\": \"integer\"\n",
    "        },\n",
    "        \"sentiment\": {\n",
    "          \"$ref\": \"#/definitions/Sentiment\"\n",
    "        },\n",
    "        \"programming_language\": {\n",
    "          \"$ref\": \"#/definitions/ProgrammingLanguage\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"question_category\",\n",
    "        \"is_off_topic\",\n",
    "        \"toxicity\",\n",
    "        \"sentiment\",\n",
    "        \"programming_language\"\n",
    "      ]\n",
    "    },\n",
    "    \"ResponseType\": {\n",
    "      \"title\": \"ResponseType\",\n",
    "      \"description\": \"An enumeration.\",\n",
    "      \"enum\": [\n",
    "        \"resolve issue\",\n",
    "        \"provide guidance\",\n",
    "        \"request information\",\n",
    "        \"give up\",\n",
    "        \"none\",\n",
    "        \"other\"\n",
    "      ],\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"ResponseCategorization\": {\n",
    "      \"title\": \"ResponseCategorization\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"response_type\": {\n",
    "          \"$ref\": \"#/definitions/ResponseType\"\n",
    "        },\n",
    "        \"response_type_if_other\": {\n",
    "          \"title\": \"Response Type If Other\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"confidence_level\": {\n",
    "          \"title\": \"Confidence Level\",\n",
    "          \"description\": \"The confidence of the assistant in its answer.\",\n",
    "          \"exclusiveMaximum\": 6,\n",
    "          \"minimum\": 0,\n",
    "          \"type\": \"integer\"\n",
    "        },\n",
    "        \"followup_actions\": {\n",
    "          \"title\": \"Followup Actions\",\n",
    "          \"description\": \"Actions the assistant recommended the user take.\",\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"response_type\",\n",
    "        \"confidence_level\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Benchmark\n",
    "\n",
    "The code following below is what we ran to ensure guidance is both expressive enough to capture structured JSON, and as a gauge on how many tokens we save by enforcing constraints. We focus on LMs that can run on a consumer device as a reflection of diverse pipeline environments.\n",
    "\n",
    "The key metrics (later found in `agg_df` as `mean_*` and `std_*` columns):\n",
    "- JSON output accuracy: `json`\n",
    "- Token reduction: `token_reduction` \n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Install benchmark dependencies (i.e. `pip install guidance[bench]`).\n",
    "2. Set environment variable `LANGCHAIN_API_KEY`. You will need an account with [LangChain](https://www.langchain.com/) to obtain a key.\n",
    "3. Accessible GGUF LM models at the below file paths. They can be found on HuggingFace\n",
    "   - [\"Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf\"](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/tree/main)\n",
    "   - [\"Llama-2-7B-32K-Instruct-GGUF/llama-2-7b-32k-instruct.Q8_0.gguf\"](https://huggingface.co/TheBloke/Llama-2-7B-32K-Instruct-GGUF/tree/main)\n",
    "   - [\"Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-fp16.gguf\"](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main)\n",
    "4. By default we assume CUDA access and high enough VRAM. Feel free to adjust the models to further quantization if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5133065e-5f03-40c5-8967-05e49439ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial checks, feel free to disable this once you have the benchmark working.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "err_msg = \"Requirements not met. Follow above instructions.\"\n",
    "try:\n",
    "    import langchain_benchmarks\n",
    "    import powerlift\n",
    "except ImportError:\n",
    "    raise ValueError(err_msg)\n",
    "if os.getenv(\"LANGCHAIN_API_KEY\") is None:\n",
    "    raise ValueError(err_msg)\n",
    "if not Path.exists(Path(\"Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf\")):\n",
    "    raise ValueError(err_msg)\n",
    "if not Path.exists(Path(\"Llama-2-7B-32K-Instruct-GGUF/llama-2-7b-32k-instruct.Q8_0.gguf\")):\n",
    "    raise ValueError(err_msg)\n",
    "if not Path.exists(Path(\"Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-fp16.gguf\")):\n",
    "    raise ValueError(err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777aaea3-cc60-4053-96e2-76f0eae6c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The JSON output benchmark is defined within the two functions below.\n",
    "\n",
    "def trial_filter(task):\n",
    "    \"\"\"This function works within our benchmarking platform to declaring which methods will be tested against what task.\n",
    "    The method names here are used later in another function `trial_runner` for conditional execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    if task.problem == \"guidance/struct_decode\":\n",
    "        return [\n",
    "            \"guidance-mistral-7b-instruct\",\n",
    "            \"base-mistral-7b-instruct\",\n",
    "            \"guidance-phi-3-mini-4k-instruct\",\n",
    "            \"base-phi-3-mini-4k-instruct\",\n",
    "            \"guidance-llama2-7b-32k-instruct\",\n",
    "            \"base-llama2-7b-32k-instruct\",\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "def trial_runner(trial):\n",
    "    \"\"\"Runs a single trial. The method to be tested will be under `trial.method.name` with the task as `trial.task.name`.\n",
    "\n",
    "    The imports and all user-defined functions are defined within. This simplifies serialization when the benchmark is run against remote machines.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    from guidance import models, gen, system, user, guidance, select, zero_or_more, capture\n",
    "    from guidance.models.transformers import Transformers\n",
    "    from guidance.models.llama_cpp import LlamaCpp\n",
    "    from guidance import json as gen_json\n",
    "    from time import time\n",
    "    import json_stream\n",
    "    import io\n",
    "    import os\n",
    "    \n",
    "    if trial.task.name == \"chat_extract\":\n",
    "        inputs, outputs, meta = trial.task.data([\"inputs\", \"outputs\", \"meta\"])\n",
    "        merged_df = pd.concat([inputs.reset_index(drop=True), outputs.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        if trial.method.name.startswith(\"guidance\"):\n",
    "            QUESTION_CAT = [\n",
    "                \"Implementation Issues\",\n",
    "                \"Feature Requests\",\n",
    "                \"Concept Explanations\",\n",
    "                \"Code Optimization\",\n",
    "                \"Security and Privacy Concerns\",\n",
    "                \"Model Training and Fine-tuning\",\n",
    "                \"Data Handling and Manipulation\",\n",
    "                \"User Interaction Flow\",\n",
    "                \"Technical Integration\",\n",
    "                \"Error Handling and Logging\",\n",
    "                \"Customization and Configuration\",\n",
    "                \"External API and Data Source Integration\",\n",
    "                \"Language and Localization\",\n",
    "                \"Streaming and Real-time Processing\",\n",
    "                \"Tool Development\",\n",
    "                \"Function Calling\",\n",
    "                \"LLM Integrations\",\n",
    "                \"General Agent Question\",\n",
    "                \"General Chit Chat\",\n",
    "                \"Memory\",\n",
    "                \"Debugging Help\",\n",
    "                \"Application Design\",\n",
    "                \"Prompt Templates\",\n",
    "                \"Cost Tracking\",\n",
    "                \"Other\"\n",
    "            ]\n",
    "            RESPONSE_TYPE = [\n",
    "                \"resolve issue\",\n",
    "                \"provide guidance\",\n",
    "                \"request information\",\n",
    "                \"give up\",\n",
    "                \"none\",\n",
    "                \"other\"\n",
    "            ]\n",
    "            \n",
    "            @guidance(stateless=True, dedent=False)\n",
    "            def guidance_list(lm):\n",
    "                return lm + \"[\" + zero_or_more(gen(regex=r'\"[\\w ]+\", ')) + gen(regex=r'\"[\\w ]+\"') + \"]\"\n",
    "            \n",
    "            WORD_PAT = r'[\\w ]+'\n",
    "            NEW_REC = '\\n            '\n",
    "            DOUBLE_QUOTE = '\"'\n",
    "            NEW_LINE = '\\n'\n",
    "            @guidance(stateless=False, dedent=False)\n",
    "            def gen_chat_json(lm):\n",
    "                lm += f\"\"\"{{\n",
    "                    \"GenerateTicket\": {{\n",
    "                        \"issue_summary\": \"{gen(regex=WORD_PAT, stop='\"')}\",\n",
    "                        \"question\": {{\n",
    "                            \"question_category\": \"{select(QUESTION_CAT, name='question_cat')}\",\n",
    "                            \"\"\"\n",
    "                if lm['question_cat'] == 'Other':\n",
    "                    lm += f\"\"\"\"category_if_other\": \"{gen(regex=WORD_PAT, stop='\"')}\",\n",
    "                            \"\"\"\n",
    "                    \n",
    "                lm += f\"\"\"\"is_off_topic\": {select([\"false\", \"true\"])},\n",
    "                            \"toxicity\": {select([0, 1, 2, 3, 4, 5])},\n",
    "                            \"sentiment\": \"{select([\"Negative\", \"Neutral\", \"Positive\"])}\",\n",
    "                            \"programming_language\": \"{select([\"python\", \"javascript\", \"typescript\", \"unknown\", \"other\"])}\"\n",
    "                        }},\n",
    "                        \"response\": {{\n",
    "                            \"response_type\": \"{select(RESPONSE_TYPE, name='response_type')}\",\n",
    "                            \"\"\"\n",
    "            \n",
    "                if lm['response_type'] == \"other\":\n",
    "                    lm += f\"\"\"\"response_type_if_other\": \"{gen(regex=WORD_PAT, stop='\"')}\",\n",
    "                            \"\"\"\n",
    "            \n",
    "                lm += f\"\"\"\"confidence_level\": {select([0, 1, 2, 3, 4, 5])}\"\"\"\n",
    "                lm += f\"\"\"{select(['', ',' + NEW_REC + '\"followup_actions\":'], name='follow_up')}\"\"\"\n",
    "                if lm.get('follow_up', None) is not None:\n",
    "                    lm += f\"\"\" {guidance_list()}\"\"\"\n",
    "            \n",
    "                lm += f\"\"\"\n",
    "                    }}\n",
    "                }}\n",
    "            }}\"\"\"\n",
    "                return lm\n",
    "\n",
    "        for i, row in merged_df.iterrows():\n",
    "            # Initialize LLM\n",
    "            if i == 0:\n",
    "                if \"mistral\" in trial.method.name:\n",
    "                    base_lm = models.LlamaCpp(\n",
    "                        \"Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf\",\n",
    "                        n_ctx=8192, n_gpu_layers=-1, echo=False, verbose=False\n",
    "                    )\n",
    "                elif \"llama2-7b\" in trial.method.name:\n",
    "                    base_lm = models.LlamaCpp(\n",
    "                        \"Llama-2-7B-32K-Instruct-GGUF/llama-2-7b-32k-instruct.Q8_0.gguf\", \n",
    "                        n_ctx=8192, n_gpu_layers=-1, echo=False, verbose=False\n",
    "                    )\n",
    "                else:\n",
    "                    base_lm = models.LlamaCpp(\n",
    "                        \"Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-fp16.gguf\", \n",
    "                        n_ctx=8192, n_gpu_layers=-1, echo=False, verbose=False\n",
    "                    )\n",
    "                    \n",
    "            # Execute LLM\n",
    "            print(f\"{trial.method.name}[{i}]\")\n",
    "            start_time = time()\n",
    "            lm = base_lm\n",
    "            lm.engine.reset_metrics()\n",
    "            if \"mistral\" in trial.method.name:\n",
    "                lm += f\"\"\"<s>[INST] {row['system_prompt']}\\n{row['user_prompt']} [/INST]\"\"\"\n",
    "            elif \"llama\" in trial.method.name:\n",
    "                lm += f\"\"\"<s>[INST] <<SYS>>\\n{row['system_prompt']}\\n<</SYS>>\\n\\n{row['user_prompt']}[/INST]\"\"\"\n",
    "            elif \"phi\" in trial.method.name:\n",
    "                lm += f\"\"\"<s><|user|>{row['system_prompt']}\\n{row['user_prompt']}<|end|><|assistant|>\"\"\"\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot support {trial.method.name} for system prompts\")\n",
    "\n",
    "            before_idx = len(str(lm))\n",
    "            if \"guidance\" in trial.method.name:\n",
    "                lm += gen_chat_json()\n",
    "            else:\n",
    "                lm += gen(max_tokens=1500)\n",
    "            output_str = str(lm)[before_idx:]\n",
    "            end_time = time()\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            # Basic measures\n",
    "            trial.log(\"output\", output_str)\n",
    "            trial.log(\"wall_time\", elapsed_time)\n",
    "\n",
    "            # Token statistics\n",
    "            tm = {\n",
    "                \"input\": lm.engine.metrics.engine_input_tokens,\n",
    "                \"output\": lm.engine.metrics.engine_output_tokens,\n",
    "                \"token_count\": lm.token_count,\n",
    "            }\n",
    "            tm[\"token_reduction\"] = 1 - (tm[\"output\"]) / (lm.token_count)\n",
    "            trial.log(\"token_input\", tm[\"input\"])\n",
    "            trial.log(\"token_output\", tm[\"output\"])\n",
    "            trial.log(\"token_count\", tm[\"token_count\"])\n",
    "            trial.log(\"token_reduction\", tm[\"token_reduction\"])\n",
    "\n",
    "            # Validate JSON conformance\n",
    "            json_success = False\n",
    "            try:\n",
    "                output_json = json.loads(output_str.strip())\n",
    "                output_json = output_json['GenerateTicket']\n",
    "                json_success = True\n",
    "            except Exception as e:\n",
    "                trial.log(\"json_errmsg\", str(e))\n",
    "            trial.log(\"json\", json_success * 1)\n",
    "\n",
    "            if json_success:\n",
    "                trial.log(\"output_json\", output_json)\n",
    "                trial.log(\"json_dirty\", 0)\n",
    "            else:\n",
    "                success = False\n",
    "                candidate = output_str.strip()\n",
    "                for i, ch in enumerate(candidate):\n",
    "                    if ch == '{':\n",
    "                        try:\n",
    "                            results = json_stream.load(io.StringIO(candidate[i:]))\n",
    "                            di = json_stream.to_standard_types(results)\n",
    "                            di = di['GenerateTicket']\n",
    "                            success = True\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if success:\n",
    "                    output_json = di\n",
    "                    trial.log(\"output_json\", output_json)\n",
    "                    trial.log(\"json_dirty\", 1)\n",
    "                else:\n",
    "                    trial.log(\"output_json\", {})\n",
    "                    trial.log(\"json_dirty\", 0)\n",
    "                \n",
    "\n",
    "            # Validate JSON schema conformance\n",
    "            from langchain_benchmarks.extraction.tasks.chat_extraction.schema import GenerateTicket\n",
    "            try:\n",
    "                GenerateTicket.parse_obj(output_json)\n",
    "                trial.log(\"json_valid\", 1)\n",
    "                if json_success:\n",
    "                    trial.log(\"json_valid_strict\", 1)\n",
    "                else:\n",
    "                    trial.log(\"json_valid_strict\", 0)\n",
    "            except Exception as e:\n",
    "                trial.log(\"json_valid\", 0)\n",
    "                trial.log(\"json_valid_strict\", 0)\n",
    "                trial.log(\"json_valid_errmsg\", str(e))\n",
    "\n",
    "            # Toxicity similarity\n",
    "            expected_json = row['output']['output']\n",
    "            expected = expected_json['question']['toxicity']\n",
    "            try:\n",
    "                pred = output_json[\"question\"][\"toxicity\"]\n",
    "                score = 1 - abs(expected - float(pred)) / 5\n",
    "                trial.log(\"toxicity\", score)\n",
    "                trial.log(\"toxicity_strict\", score)\n",
    "            except Exception as e:\n",
    "                trial.log(\"toxicity_strict\", 0)\n",
    "                trial.log(\"toxicity_errmsg\", str(e))\n",
    "\n",
    "            # Sentiment similarity\n",
    "            expected =  expected_json[\"question\"][\"sentiment\"]\n",
    "            ordinal_map = {\n",
    "                \"negative\": 0,\n",
    "                \"neutral\": 1,\n",
    "                \"positive\": 2,\n",
    "            }\n",
    "            expected_score = ordinal_map.get(str(expected).lower())\n",
    "            try:\n",
    "                pred = output_json[\"question\"][\"sentiment\"]\n",
    "                pred_score = ordinal_map.get(str(pred).lower())\n",
    "                score = 1 - (abs(expected_score - float(pred_score)) / 2)\n",
    "                trial.log(\"sentiment\", score)\n",
    "                trial.log(\"sentiment_strict\", score)\n",
    "            except Exception as e:\n",
    "                trial.log(\"sentiment_strict\", 0)\n",
    "                trial.log(\"sentiment_errmsg\", str(e))\n",
    "\n",
    "            # Question category similarity\n",
    "            expected = expected_json[\"question\"][\"question_category\"]\n",
    "            try:\n",
    "                pred = output_json[\"question\"][\"question_category\"]\n",
    "                score = int(expected == pred)\n",
    "                trial.log(\"question_cat\", score)\n",
    "                trial.log(\"question_cat_strict\", score)\n",
    "            except Exception as e:\n",
    "                trial.log(\"question_cat_strict\", 0)\n",
    "                trial.log(\"question_cat_errmsg\", str(e))\n",
    "\n",
    "            # Off-topic similarity\n",
    "            expected = expected_json[\"question\"][\"is_off_topic\"]\n",
    "            try:\n",
    "                pred = output_json[\"question\"].get(\"is_off_topic\")\n",
    "                score = int(expected == pred)\n",
    "                trial.log(\"offtopic\", score)\n",
    "                trial.log(\"offtopic_strict\", score)\n",
    "            except Exception as e:\n",
    "                trial.log(\"offtopic_strict\", 0)\n",
    "                trial.log(\"offtopic_errmsg\", str(e))\n",
    "\n",
    "            # Programming language similarity\n",
    "            expected = expected_json[\"question\"][\"programming_language\"]\n",
    "            try:\n",
    "                pred = output_json[\"question\"][\"programming_language\"]\n",
    "                score = int(expected == pred)\n",
    "                trial.log(\"programming\", score)\n",
    "                trial.log(\"programming_strict\", score)\n",
    "            except Exception as e:\n",
    "                trial.log(\"programming_strict\", 0)\n",
    "                trial.log(\"programming_errmsg\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5752400f-ce25-40f6-84d5-e166ff2e7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Chat Extraction already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/7953c7d1-5ab3-5e87-9b8d-ca40eef5f42d/datasets/cf39229b-5aa6-4ffa-a7a4-effe91894d12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<powerlift.executors.localmachine.LocalMachine at 0x7f148499f460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guidance-mistral-7b-instruct[0]\n",
      "guidance-mistral-7b-instruct[1]\n",
      "guidance-mistral-7b-instruct[2]\n",
      "guidance-mistral-7b-instruct[3]\n",
      "guidance-mistral-7b-instruct[4]\n",
      "guidance-mistral-7b-instruct[5]\n",
      "guidance-mistral-7b-instruct[6]\n",
      "guidance-mistral-7b-instruct[7]\n",
      "guidance-mistral-7b-instruct[8]\n",
      "guidance-mistral-7b-instruct[9]\n",
      "guidance-mistral-7b-instruct[10]\n",
      "guidance-mistral-7b-instruct[11]\n",
      "guidance-mistral-7b-instruct[12]\n",
      "guidance-mistral-7b-instruct[13]\n",
      "guidance-mistral-7b-instruct[14]\n",
      "guidance-mistral-7b-instruct[15]\n",
      "guidance-mistral-7b-instruct[16]\n",
      "guidance-mistral-7b-instruct[17]\n",
      "guidance-mistral-7b-instruct[18]\n",
      "guidance-mistral-7b-instruct[19]\n",
      "guidance-mistral-7b-instruct[20]\n",
      "guidance-mistral-7b-instruct[21]\n",
      "guidance-mistral-7b-instruct[22]\n",
      "guidance-mistral-7b-instruct[23]\n",
      "guidance-mistral-7b-instruct[24]\n",
      "guidance-mistral-7b-instruct[25]\n",
      "guidance-mistral-7b-instruct[26]\n",
      "base-mistral-7b-instruct[0]\n",
      "base-mistral-7b-instruct[1]\n",
      "base-mistral-7b-instruct[2]\n",
      "base-mistral-7b-instruct[3]\n",
      "base-mistral-7b-instruct[4]\n",
      "base-mistral-7b-instruct[5]\n",
      "base-mistral-7b-instruct[6]\n",
      "base-mistral-7b-instruct[7]\n",
      "base-mistral-7b-instruct[8]\n",
      "base-mistral-7b-instruct[9]\n",
      "base-mistral-7b-instruct[10]\n",
      "base-mistral-7b-instruct[11]\n",
      "base-mistral-7b-instruct[12]\n",
      "base-mistral-7b-instruct[13]\n",
      "base-mistral-7b-instruct[14]\n",
      "base-mistral-7b-instruct[15]\n",
      "base-mistral-7b-instruct[16]\n",
      "base-mistral-7b-instruct[17]\n",
      "base-mistral-7b-instruct[18]\n",
      "base-mistral-7b-instruct[19]\n",
      "base-mistral-7b-instruct[20]\n",
      "base-mistral-7b-instruct[21]\n",
      "base-mistral-7b-instruct[22]\n",
      "base-mistral-7b-instruct[23]\n",
      "base-mistral-7b-instruct[24]\n",
      "base-mistral-7b-instruct[25]\n",
      "base-mistral-7b-instruct[26]\n",
      "guidance-phi-3-mini-4k-instruct[0]\n",
      "guidance-phi-3-mini-4k-instruct[1]\n",
      "guidance-phi-3-mini-4k-instruct[2]\n",
      "guidance-phi-3-mini-4k-instruct[3]\n",
      "guidance-phi-3-mini-4k-instruct[4]\n",
      "guidance-phi-3-mini-4k-instruct[5]\n",
      "guidance-phi-3-mini-4k-instruct[6]\n",
      "guidance-phi-3-mini-4k-instruct[7]\n",
      "guidance-phi-3-mini-4k-instruct[8]\n",
      "guidance-phi-3-mini-4k-instruct[9]\n",
      "guidance-phi-3-mini-4k-instruct[10]\n",
      "guidance-phi-3-mini-4k-instruct[11]\n",
      "guidance-phi-3-mini-4k-instruct[12]\n",
      "guidance-phi-3-mini-4k-instruct[13]\n",
      "guidance-phi-3-mini-4k-instruct[14]\n",
      "guidance-phi-3-mini-4k-instruct[15]\n",
      "guidance-phi-3-mini-4k-instruct[16]\n",
      "guidance-phi-3-mini-4k-instruct[17]\n",
      "guidance-phi-3-mini-4k-instruct[18]\n",
      "guidance-phi-3-mini-4k-instruct[19]\n",
      "guidance-phi-3-mini-4k-instruct[20]\n",
      "guidance-phi-3-mini-4k-instruct[21]\n",
      "guidance-phi-3-mini-4k-instruct[22]\n",
      "guidance-phi-3-mini-4k-instruct[23]\n",
      "guidance-phi-3-mini-4k-instruct[24]\n",
      "guidance-phi-3-mini-4k-instruct[25]\n",
      "guidance-phi-3-mini-4k-instruct[26]\n",
      "base-phi-3-mini-4k-instruct[0]\n",
      "base-phi-3-mini-4k-instruct[1]\n",
      "base-phi-3-mini-4k-instruct[2]\n",
      "base-phi-3-mini-4k-instruct[3]\n",
      "base-phi-3-mini-4k-instruct[4]\n",
      "base-phi-3-mini-4k-instruct[5]\n",
      "base-phi-3-mini-4k-instruct[6]\n",
      "base-phi-3-mini-4k-instruct[7]\n",
      "base-phi-3-mini-4k-instruct[8]\n",
      "base-phi-3-mini-4k-instruct[9]\n",
      "base-phi-3-mini-4k-instruct[10]\n",
      "base-phi-3-mini-4k-instruct[11]\n",
      "base-phi-3-mini-4k-instruct[12]\n",
      "base-phi-3-mini-4k-instruct[13]\n",
      "base-phi-3-mini-4k-instruct[14]\n",
      "base-phi-3-mini-4k-instruct[15]\n",
      "base-phi-3-mini-4k-instruct[16]\n",
      "base-phi-3-mini-4k-instruct[17]\n",
      "base-phi-3-mini-4k-instruct[18]\n",
      "base-phi-3-mini-4k-instruct[19]\n",
      "base-phi-3-mini-4k-instruct[20]\n",
      "base-phi-3-mini-4k-instruct[21]\n",
      "base-phi-3-mini-4k-instruct[22]\n",
      "base-phi-3-mini-4k-instruct[23]\n",
      "base-phi-3-mini-4k-instruct[24]\n",
      "base-phi-3-mini-4k-instruct[25]\n",
      "base-phi-3-mini-4k-instruct[26]\n",
      "guidance-llama2-7b-32k-instruct[0]\n",
      "guidance-llama2-7b-32k-instruct[1]\n",
      "guidance-llama2-7b-32k-instruct[2]\n",
      "guidance-llama2-7b-32k-instruct[3]\n",
      "guidance-llama2-7b-32k-instruct[4]\n",
      "guidance-llama2-7b-32k-instruct[5]\n",
      "guidance-llama2-7b-32k-instruct[6]\n",
      "guidance-llama2-7b-32k-instruct[7]\n",
      "guidance-llama2-7b-32k-instruct[8]\n",
      "guidance-llama2-7b-32k-instruct[9]\n",
      "guidance-llama2-7b-32k-instruct[10]\n",
      "guidance-llama2-7b-32k-instruct[11]\n",
      "guidance-llama2-7b-32k-instruct[12]\n",
      "guidance-llama2-7b-32k-instruct[13]\n",
      "guidance-llama2-7b-32k-instruct[14]\n",
      "guidance-llama2-7b-32k-instruct[15]\n",
      "guidance-llama2-7b-32k-instruct[16]\n",
      "guidance-llama2-7b-32k-instruct[17]\n",
      "guidance-llama2-7b-32k-instruct[18]\n",
      "guidance-llama2-7b-32k-instruct[19]\n",
      "guidance-llama2-7b-32k-instruct[20]\n",
      "guidance-llama2-7b-32k-instruct[21]\n",
      "guidance-llama2-7b-32k-instruct[22]\n",
      "guidance-llama2-7b-32k-instruct[23]\n",
      "guidance-llama2-7b-32k-instruct[24]\n",
      "guidance-llama2-7b-32k-instruct[25]\n",
      "guidance-llama2-7b-32k-instruct[26]\n",
      "base-llama2-7b-32k-instruct[0]\n",
      "base-llama2-7b-32k-instruct[1]\n",
      "base-llama2-7b-32k-instruct[2]\n",
      "base-llama2-7b-32k-instruct[3]\n",
      "base-llama2-7b-32k-instruct[4]\n",
      "base-llama2-7b-32k-instruct[5]\n",
      "base-llama2-7b-32k-instruct[6]\n",
      "base-llama2-7b-32k-instruct[7]\n",
      "base-llama2-7b-32k-instruct[8]\n",
      "base-llama2-7b-32k-instruct[9]\n",
      "base-llama2-7b-32k-instruct[10]\n",
      "base-llama2-7b-32k-instruct[11]\n",
      "base-llama2-7b-32k-instruct[12]\n",
      "base-llama2-7b-32k-instruct[13]\n",
      "base-llama2-7b-32k-instruct[14]\n",
      "base-llama2-7b-32k-instruct[15]\n",
      "base-llama2-7b-32k-instruct[16]\n",
      "base-llama2-7b-32k-instruct[17]\n",
      "base-llama2-7b-32k-instruct[18]\n",
      "base-llama2-7b-32k-instruct[19]\n",
      "base-llama2-7b-32k-instruct[20]\n",
      "base-llama2-7b-32k-instruct[21]\n",
      "base-llama2-7b-32k-instruct[22]\n",
      "base-llama2-7b-32k-instruct[23]\n",
      "base-llama2-7b-32k-instruct[24]\n",
      "base-llama2-7b-32k-instruct[25]\n",
      "base-llama2-7b-32k-instruct[26]\n"
     ]
    }
   ],
   "source": [
    "# Run the benchmark. This is asynchronous so it should return relatively quickly.\n",
    "# By default, before each method is run on an example, it will print its name and the example index as specified in `trial_runner`.\n",
    "\n",
    "from powerlift.bench import Benchmark, Store, populate_with_datasets\n",
    "from powerlift.executors import LocalMachine\n",
    "from guidance.bench import retrieve_langchain\n",
    "from pathlib import Path\n",
    "\n",
    "conn_str = f\"sqlite:///{Path(Path.cwd(), 'guidance-bench.db')}\"\n",
    "store = Store(conn_str, force_recreate=False)\n",
    "populate_with_datasets(store, retrieve_langchain(cache_dir=\"~/.guidance-bench/cache\"), exist_ok=True)\n",
    "executor = LocalMachine(store, n_cpus=1, debug_mode=False)\n",
    "\n",
    "bench = Benchmark(store, name=\"local_lm_chat_extract\")\n",
    "bench.run(trial_runner, trial_filter, timeout=60*60, executor=executor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5578e173-fe86-48d1-8329-33e040526fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>replicate_num</th>\n",
       "      <th>meta</th>\n",
       "      <th>method</th>\n",
       "      <th>task</th>\n",
       "      <th>status</th>\n",
       "      <th>errmsg</th>\n",
       "      <th>create_time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>guidance-mistral-7b-instruct</td>\n",
       "      <td>chat_extract</td>\n",
       "      <td>READY</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-17 20:45:29.721393</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>base-mistral-7b-instruct</td>\n",
       "      <td>chat_extract</td>\n",
       "      <td>READY</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-17 20:45:29.721516</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>guidance-phi-3-mini-4k-instruct</td>\n",
       "      <td>chat_extract</td>\n",
       "      <td>READY</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-17 20:45:29.721562</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>base-phi-3-mini-4k-instruct</td>\n",
       "      <td>chat_extract</td>\n",
       "      <td>READY</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-17 20:45:29.721599</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>guidance-llama2-7b-32k-instruct</td>\n",
       "      <td>chat_extract</td>\n",
       "      <td>READY</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-17 20:45:29.721636</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>base-llama2-7b-32k-instruct</td>\n",
       "      <td>chat_extract</td>\n",
       "      <td>READY</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-05-17 20:45:29.721672</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_id  replicate_num meta                           method  \\\n",
       "0         1              0   {}     guidance-mistral-7b-instruct   \n",
       "1         2              0   {}         base-mistral-7b-instruct   \n",
       "2         3              0   {}  guidance-phi-3-mini-4k-instruct   \n",
       "3         4              0   {}      base-phi-3-mini-4k-instruct   \n",
       "4         5              0   {}  guidance-llama2-7b-32k-instruct   \n",
       "5         6              0   {}      base-llama2-7b-32k-instruct   \n",
       "\n",
       "           task status errmsg                create_time start_time end_time  \n",
       "0  chat_extract  READY   None 2024-05-17 20:45:29.721393       None     None  \n",
       "1  chat_extract  READY   None 2024-05-17 20:45:29.721516       None     None  \n",
       "2  chat_extract  READY   None 2024-05-17 20:45:29.721562       None     None  \n",
       "3  chat_extract  READY   None 2024-05-17 20:45:29.721599       None     None  \n",
       "4  chat_extract  READY   None 2024-05-17 20:45:29.721636       None     None  \n",
       "5  chat_extract  READY   None 2024-05-17 20:45:29.721672       None     None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the status of the trial runs. Status column will be set to 'COMPLETE' for all tasks at the end.\n",
    "bench.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94caf36-5634-4cdd-8234-5d67fae41cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_json</th>\n",
       "      <th>mean_json_dirty</th>\n",
       "      <th>mean_json_valid</th>\n",
       "      <th>mean_json_valid_strict</th>\n",
       "      <th>mean_offtopic</th>\n",
       "      <th>mean_offtopic_strict</th>\n",
       "      <th>mean_programming</th>\n",
       "      <th>mean_programming_strict</th>\n",
       "      <th>mean_question_cat</th>\n",
       "      <th>mean_question_cat_strict</th>\n",
       "      <th>...</th>\n",
       "      <th>std_question_cat_strict</th>\n",
       "      <th>std_sentiment</th>\n",
       "      <th>std_sentiment_strict</th>\n",
       "      <th>std_token_count</th>\n",
       "      <th>std_token_input</th>\n",
       "      <th>std_token_output</th>\n",
       "      <th>std_token_reduction</th>\n",
       "      <th>std_toxicity</th>\n",
       "      <th>std_toxicity_strict</th>\n",
       "      <th>std_wall_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base-llama2-7b-32k-instruct</th>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.363059</td>\n",
       "      <td>440.643091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.046848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base-mistral-7b-instruct</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26087</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320256</td>\n",
       "      <td>387.916261</td>\n",
       "      <td>681.601341</td>\n",
       "      <td>387.679932</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10.971967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base-phi-3-mini-4k-instruct</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.314600</td>\n",
       "      <td>438.717735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.876374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guidance-llama2-7b-32k-instruct</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>191.509285</td>\n",
       "      <td>465.816197</td>\n",
       "      <td>191.311160</td>\n",
       "      <td>0.141899</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.480775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guidance-mistral-7b-instruct</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320256</td>\n",
       "      <td>0.133440</td>\n",
       "      <td>0.133440</td>\n",
       "      <td>5.995725</td>\n",
       "      <td>442.744802</td>\n",
       "      <td>4.798504</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.577206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guidance-phi-3-mini-4k-instruct</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266880</td>\n",
       "      <td>0.197924</td>\n",
       "      <td>0.197924</td>\n",
       "      <td>15.927542</td>\n",
       "      <td>438.253725</td>\n",
       "      <td>11.748431</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.03849</td>\n",
       "      <td>0.03849</td>\n",
       "      <td>0.356682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mean_json mean_json_dirty mean_json_valid  \\\n",
       "method                                                                      \n",
       "base-llama2-7b-32k-instruct      0.259259             0.0             0.0   \n",
       "base-mistral-7b-instruct              0.0        0.814815             0.0   \n",
       "base-phi-3-mini-4k-instruct           0.0        0.148148             0.0   \n",
       "guidance-llama2-7b-32k-instruct       1.0             0.0             1.0   \n",
       "guidance-mistral-7b-instruct          1.0             0.0             1.0   \n",
       "guidance-phi-3-mini-4k-instruct       1.0             0.0             1.0   \n",
       "\n",
       "                                mean_json_valid_strict mean_offtopic  \\\n",
       "method                                                                 \n",
       "base-llama2-7b-32k-instruct                        0.0           0.0   \n",
       "base-mistral-7b-instruct                           0.0           0.0   \n",
       "base-phi-3-mini-4k-instruct                        0.0           0.0   \n",
       "guidance-llama2-7b-32k-instruct                    1.0      0.888889   \n",
       "guidance-mistral-7b-instruct                       1.0      0.888889   \n",
       "guidance-phi-3-mini-4k-instruct                    1.0      0.851852   \n",
       "\n",
       "                                mean_offtopic_strict mean_programming  \\\n",
       "method                                                                  \n",
       "base-llama2-7b-32k-instruct                      0.0              NaN   \n",
       "base-mistral-7b-instruct                         0.0              NaN   \n",
       "base-phi-3-mini-4k-instruct                      0.0              NaN   \n",
       "guidance-llama2-7b-32k-instruct             0.888889         0.296296   \n",
       "guidance-mistral-7b-instruct                0.888889          0.62963   \n",
       "guidance-phi-3-mini-4k-instruct             0.851852         0.259259   \n",
       "\n",
       "                                mean_programming_strict mean_question_cat  \\\n",
       "method                                                                      \n",
       "base-llama2-7b-32k-instruct                         0.0               NaN   \n",
       "base-mistral-7b-instruct                            0.0           0.26087   \n",
       "base-phi-3-mini-4k-instruct                         0.0               NaN   \n",
       "guidance-llama2-7b-32k-instruct                0.296296          0.074074   \n",
       "guidance-mistral-7b-instruct                    0.62963          0.111111   \n",
       "guidance-phi-3-mini-4k-instruct                0.259259          0.074074   \n",
       "\n",
       "                                mean_question_cat_strict  ...  \\\n",
       "method                                                    ...   \n",
       "base-llama2-7b-32k-instruct                          0.0  ...   \n",
       "base-mistral-7b-instruct                        0.222222  ...   \n",
       "base-phi-3-mini-4k-instruct                          0.0  ...   \n",
       "guidance-llama2-7b-32k-instruct                 0.074074  ...   \n",
       "guidance-mistral-7b-instruct                    0.111111  ...   \n",
       "guidance-phi-3-mini-4k-instruct                 0.074074  ...   \n",
       "\n",
       "                                std_question_cat_strict std_sentiment  \\\n",
       "method                                                                  \n",
       "base-llama2-7b-32k-instruct                    0.000000           NaN   \n",
       "base-mistral-7b-instruct                       0.423659      0.000000   \n",
       "base-phi-3-mini-4k-instruct                    0.000000           NaN   \n",
       "guidance-llama2-7b-32k-instruct                0.266880      0.000000   \n",
       "guidance-mistral-7b-instruct                   0.320256      0.133440   \n",
       "guidance-phi-3-mini-4k-instruct                0.266880      0.197924   \n",
       "\n",
       "                                std_sentiment_strict std_token_count  \\\n",
       "method                                                                 \n",
       "base-llama2-7b-32k-instruct                 0.000000        2.363059   \n",
       "base-mistral-7b-instruct                    0.320256      387.916261   \n",
       "base-phi-3-mini-4k-instruct                 0.000000       58.314600   \n",
       "guidance-llama2-7b-32k-instruct             0.000000      191.509285   \n",
       "guidance-mistral-7b-instruct                0.133440        5.995725   \n",
       "guidance-phi-3-mini-4k-instruct             0.197924       15.927542   \n",
       "\n",
       "                                std_token_input std_token_output  \\\n",
       "method                                                             \n",
       "base-llama2-7b-32k-instruct          440.643091         0.000000   \n",
       "base-mistral-7b-instruct             681.601341       387.679932   \n",
       "base-phi-3-mini-4k-instruct          438.717735         0.000000   \n",
       "guidance-llama2-7b-32k-instruct      465.816197       191.311160   \n",
       "guidance-mistral-7b-instruct         442.744802         4.798504   \n",
       "guidance-phi-3-mini-4k-instruct      438.253725        11.748431   \n",
       "\n",
       "                                std_token_reduction std_toxicity  \\\n",
       "method                                                             \n",
       "base-llama2-7b-32k-instruct                0.001582          NaN   \n",
       "base-mistral-7b-instruct                   0.004073          NaN   \n",
       "base-phi-3-mini-4k-instruct                0.048805          NaN   \n",
       "guidance-llama2-7b-32k-instruct            0.141899      0.00000   \n",
       "guidance-mistral-7b-instruct               0.025946      0.00000   \n",
       "guidance-phi-3-mini-4k-instruct            0.052191      0.03849   \n",
       "\n",
       "                                std_toxicity_strict  std_wall_time  \n",
       "method                                                              \n",
       "base-llama2-7b-32k-instruct                 0.00000       6.046848  \n",
       "base-mistral-7b-instruct                    0.00000      10.971967  \n",
       "base-phi-3-mini-4k-instruct                 0.00000       2.876374  \n",
       "guidance-llama2-7b-32k-instruct             0.00000      11.480775  \n",
       "guidance-mistral-7b-instruct                0.00000       0.577206  \n",
       "guidance-phi-3-mini-4k-instruct             0.03849       0.356682  \n",
       "\n",
       "[6 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate results, we wait for the benchmark first to complete.\n",
    "# The most important measures are `mean_json` and `mean_token_reduction`.\n",
    "# Negative values can happen with `mean_token_reduction`, this is expected due to token healing complicating how we count tokens.\n",
    "\n",
    "bench.wait_until_complete()\n",
    "result_df = bench.results()\n",
    "agg_df = result_df.pivot_table(index='method', columns='name', values='num_val', aggfunc=['mean', 'std'])\n",
    "agg_df.columns = [\"_\".join(x) for x in agg_df.columns.to_flat_index()]\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8c6c8-f0ce-42ee-85f8-bc2bbecd2009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
