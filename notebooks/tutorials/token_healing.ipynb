{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token healing\n",
    "\n",
    "Guidance uses what we call \"token healing\" to fix tokenization artifacts that normally arise at the boundary between the end of a prompt and the beginning of a set of generated tokens. Note that token healing requires direct endpoint integration to run effciently, so it currently supported only for the `guidance.models.LlamaCpp` and `guidance.models.Transformers` LLM backends.\n",
    "\n",
    "## Why token healing is needed\n",
    "Language models process tokens, which are chunks of text that often are similar to a word. This impacts how language models see text, and also how we can prompt them, since every prompt has to be a set of tokens. Encodings like [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) that are used by GPT-style models map all input bytes to token ids in an optimized manner. This works well during training, but can lead to some subtle issues during prompting and inference because the token boundaries often don't line up with the end of the prompt if we consider the generated tokens that will also come next. Of course the end of a prompt will always align with a token boundary because the prompt is tokenized before being extended by the model, but if the first characters of the completion are part of a longer token that would span the prompt boundary, that longer token cannot be used (even though that is what the model would expect based on training data). \n",
    "\n",
    "To see why token healing is important consider the prompt \"This is a \", which is then completed with \"fine day.\" by the model, so resulting in the final string \"This is a fine day.\". If we tokenize the prompt \"This is a \" with GPT2 BPE we get `[1212, 318, 257, 220]`, and the tokenization of the extention \"fine day.\" is `[38125, 1110, 13]`. This results in a final prompt + generation token sequence of `[1212, 318, 257, 220, 38125, 1110, 13]`. If however we were to tokenize the whole string \"This is a fine day.\" jointly we instead get `[1212, 318, 257, 3734, 1110, 13]`. Which tokenization is correct? Well, the correct tokenization is the one that best communicates intent to the model. Since the model learned intent based on a optimized tokenization of the training text, that means the joint tokenization that also uses optimized matching will better align with how the model processed the training data, and so it is also likely to better communicate intent to the model. This is the reason why ending your prompt with a space is almost always a bad idea in GPT models since most word-based tokens have the space before the word, not after it.\n",
    "\n",
    "Note that another way to see that the \"standard\" prompt-boundary-based encoding is worse than the joint one we get with token healing is to observe that 38125 (the token id for \"fine\") is a large number, this means it is uncommon to see that token in the training data (since BPE encodings are built up greedily based on frequency). In contrast 3734 (the token id for \" fine\") is a much more common token and so more likely to clearly communicate intent to the model (since the model has seen it many times and hence had more opportunity to learn its meaning in many contexts).\n",
    "\n",
    "## How token healing works\n",
    "Guidance avoids the above tokenization artifacts automatically using a method we call \"token healing\" that backs up the generation process by one or more tokens before the end of the prompt, then constrains the first tokens generated to have a prefix that matches the last token in the prompt. This allows the generated text string to have the token encoding that the model would expect based on its training data, not an unusual alternative encoding forcing by the prompt boundary. Token healing allows you to express your prompts however you wish, without worrying about boundaries (which effect many tokens, not just space characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of `This is a `: [1212, 318, 257, 220]\n",
      "Tokenization of `fine day.`: [38125, 1110, 13]\n",
      "Tokenization of `This is a fine day.`: [1212, 318, 257, 3734, 1110, 13]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# compute the tokenizations of the example above\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "print(\"Tokenization of `This is a `:\", tokenizer.encode(\"This is a \"))\n",
    "print(\"Tokenization of `fine day.`:\", tokenizer.encode(\"fine day.\"))\n",
    "print(\"Tokenization of `This is a fine day.`:\", tokenizer.encode(\"This is a fine day.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token healing in action\n",
    "\n",
    "Below is a prompt that we run both with and without token healing to see how it can impact generation quality.\n",
    "\n",
    "### With token healing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riedgar\\AppData\\Local\\miniconda3\\envs\\guidance-312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\riedgar\\AppData\\Local\\miniconda3\\envs\\guidance-312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "gpustat is not installed, run `pip install gpustat` to collect GPU stats.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd83db5a8494c709f807d7ae1ed8704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import guidance\n",
    "from guidance import gen, models\n",
    "\n",
    "gpt2 = models.Transformers(\"gpt2\", temperature=0.8)\n",
    "\n",
    "gpt2 += \"The url of Google is http:\" + gen(max_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without token healing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The url of Google is http: //www.google.com/search?q=google+'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = transformers.pipeline('text-generation', model='gpt2')\n",
    "\n",
    "generator(\"The url of Google is http:\", max_length=20, temperature=0.0001)[0][\"generated_text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may seems strange to us why GPT2 does not put a \"//\" after the colon character (instead writing a space), it makes sense if we think about the tokens involved. If a \"//\" was likely to come after the space, then it would have been included using the token \"://\". By sending the token id 25 (a colon) by itself to the model we are communicating that what comes next is not something that is in a token that starts with a colon (since otherwise the greedy/optimized tokenization would have used it). So GPT2 picks something that cannot be consumed into a larger token, a space character (since \": \" is not a GPT2 token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([25], [25, 220], [1378])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\":\"), tokenizer.encode(\": \"), tokenizer.encode(\"://\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
    "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
