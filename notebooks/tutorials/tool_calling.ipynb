{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213de727",
   "metadata": {},
   "source": [
    "# Defining Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f35cc5",
   "metadata": {},
   "source": [
    "A `Tool` consists of:\n",
    "1. A name that is used to uniquely identify the tool so the LM can specify which tool it intends to call\n",
    "2. A description that is used to help indicate to the LM when it is appropriate to call the tool\n",
    "3. A callable that is executed when the tool is actually called\n",
    "4. A constraint that ensures that the LM's *output* is a valid *input* to the callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124195a",
   "metadata": {},
   "source": [
    "## Functions\n",
    "The most common type of tool is defined by a function.\n",
    "\n",
    "The constraint on the LM's output can be specified by a JSON schema, where the JSON schema should have type `object` and have `properties` matching the names of the function's arguments.\n",
    "\n",
    "Note: positional-only and variadic arguments are not yet supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cc9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "\n",
    "def get_weather_func(city: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given city.\n",
    "    \"\"\"\n",
    "    temp = random.randint(-10, 35)\n",
    "    if temp <= 0:\n",
    "        weather = \"snowy\"\n",
    "    elif temp <= 20:\n",
    "        weather = \"cloudy\"\n",
    "    else:\n",
    "        weather = \"sunny\"\n",
    "    if unit == \"fahrenheit\":\n",
    "        temp = temp * 9/5 + 32\n",
    "    return f\"The current temperature in {city} is {temp} degrees {unit} and it is {weather}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0669f5",
   "metadata": {},
   "source": [
    "`Tool.from_callable` takes a function and creates a `Tool` object.\n",
    "\n",
    "- The name of the tool will be inferred from the function's name\n",
    "- The description of the tool will be inferred from the function's docstring\n",
    "- The JSON schema of the parameters will be inferred from any type annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6219b79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_weather_func',\n",
       " 'description': 'Get the current weather for a given city.',\n",
       " 'tool': {'type': 'function',\n",
       "  'parameters': {'additionalProperties': False,\n",
       "   'properties': {'city': {'title': 'City', 'type': 'string'},\n",
       "    'unit': {'enum': ['celsius', 'fahrenheit'],\n",
       "     'title': 'Unit',\n",
       "     'type': 'string'}},\n",
       "   'required': ['city', 'unit'],\n",
       "   'title': 'get_weather_func',\n",
       "   'type': 'object'}},\n",
       " 'callable': <function __main__.get_weather_func(city: str, unit: Literal['celsius', 'fahrenheit'] = 'celsius') -> str>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from guidance.types import Tool\n",
    "\n",
    "Tool.from_callable(\n",
    "    callable=get_weather_func\n",
    ").model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8d745",
   "metadata": {},
   "source": [
    "The name, description, and parameters (may be a JSON schema or a pydantic.BaseModel class) can optionally be specified directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b044ab9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_weather',\n",
       " 'description': 'Get the current weather for a given city.',\n",
       " 'tool': {'type': 'function',\n",
       "  'parameters': {'additionalProperties': False,\n",
       "   'properties': {'city': {'description': 'The name of the city to get the weather for.',\n",
       "     'title': 'City',\n",
       "     'type': 'string'},\n",
       "    'unit': {'default': 'celsius',\n",
       "     'description': 'The unit of temperature to return.',\n",
       "     'enum': ['celsius', 'fahrenheit'],\n",
       "     'title': 'Unit',\n",
       "     'type': 'string'}},\n",
       "   'required': ['city'],\n",
       "   'title': 'GetWeatherArgs',\n",
       "   'type': 'object'}},\n",
       " 'callable': <function __main__.get_weather_func(city: str, unit: Literal['celsius', 'fahrenheit'] = 'celsius') -> str>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"The name of the city to get the weather for.\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(\"celsius\", description=\"The unit of temperature to return.\")\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra=\"forbid\", # Disallow extra fields -- required for OpenAI\n",
    "    )\n",
    "\n",
    "get_weather_tool = Tool.from_callable(\n",
    "    callable=get_weather_func,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a given city.\",\n",
    "    parameters=GetWeatherArgs\n",
    ")\n",
    "get_weather_tool.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d4e0b",
   "metadata": {},
   "source": [
    "## Custom Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b96a80",
   "metadata": {},
   "source": [
    "### Regex tools\n",
    "We can use a regex to define the type of structured input that our tool expects. In this example, our tool is a `whois` lookup that expects a well-formatted url.\n",
    "\n",
    "Note that unlike `Tool.from_callable`, the callable passed to `Tool.from_regex` must take exactly one argument, which will be passed as a *string matching the given regex*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efdd015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'whois_lookup',\n",
       " 'description': 'A tool that performs a WHOIS lookup on a domain.',\n",
       " 'tool': {'type': 'custom',\n",
       "  'format': {'type': 'grammar',\n",
       "   'syntax': 'regex',\n",
       "   'definition': 'https?:\\\\/\\\\/[^\\\\s]+'}},\n",
       " 'callable': <function __main__.whois_lookup(url: str) -> str>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def whois_lookup(url: str) -> str:\n",
    "    domain = urlparse(url).netloc\n",
    "    result = subprocess.run([\"whois\", domain], capture_output=True, text=True, timeout=30)\n",
    "    return result.stdout\n",
    "\n",
    "whois_tool = Tool.from_regex(\n",
    "    pattern=r\"https?:\\/\\/[^\\s]+\",\n",
    "    name=\"whois_lookup\",\n",
    "    description=\"A tool that performs a WHOIS lookup on a domain.\",\n",
    "    callable=whois_lookup\n",
    ")\n",
    "whois_tool.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3619d85",
   "metadata": {},
   "source": [
    "### Guidance grammar tools\n",
    "In addition to regex tools, we can define guidance grammars that constrain the model's output to match our tool's expected input format.\n",
    "\n",
    "Below, we define a grammar that describes simple arithmetic expressions, and our tool will evaluate these expressions as python numbers.\n",
    "\n",
    "Again, note that the callable passed to `Tool.from_grammar` must take exactly one argument, which will be passed as a *string matching the given grammar*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7a11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import guidance, select, regex\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def expression(lm):\n",
    "    return lm + select([\n",
    "        term(),\n",
    "        expression() + \"+\" + term(),\n",
    "        expression() + \"-\" + term(),\n",
    "    ])\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def term(lm):\n",
    "    return lm + select([\n",
    "        factor(),\n",
    "        term() + \"*\" + factor(),\n",
    "        term() + \"/\" + factor(),\n",
    "    ])\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def factor(lm):\n",
    "    return lm + select([number(), \"(\" + expression() + \")\"])\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def number(lm):\n",
    "    return lm + regex(r'\\d+(\\.\\d+)?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01edab91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'calculator',\n",
       " 'description': 'A calculator that can evaluate mathematical expressions.',\n",
       " 'tool': {'type': 'custom',\n",
       "  'format': {'type': 'grammar',\n",
       "   'syntax': 'lark',\n",
       "   'definition': '%llguidance {}\\n\\nstart: expression\\n\\nexpression: term\\n     | expression \"+\" term\\n     | expression \"-\" term\\n\\nterm: factor\\n     | term \"*\" factor\\n     | term \"/\" factor\\n\\nfactor: NUMBER\\n     | \"(\" expression \")\"\\n\\nNUMBER: /\\\\d+(\\\\.\\\\d+)?/\\n'}},\n",
       " 'callable': <function __main__.evaluate_expression(expr: str) -> Union[int, float]>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import sympify\n",
    "from ast import literal_eval\n",
    "from typing import Union\n",
    "\n",
    "def evaluate_expression(expr: str) -> Union[int, float]:\n",
    "    sympy_expr = sympify(expr)\n",
    "    if not sympy_expr.is_number:\n",
    "        raise ValueError(f\"Invalid expression: {expr}\")\n",
    "    return literal_eval(str(sympy_expr))\n",
    "\n",
    "calculator = Tool.from_grammar(\n",
    "    grammar=expression(),\n",
    "    name=\"calculator\",\n",
    "    description=\"A calculator that can evaluate mathematical expressions.\",\n",
    "    callable=evaluate_expression\n",
    ")\n",
    "calculator.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d0caf",
   "metadata": {},
   "source": [
    "### Lark tools\n",
    "Under the hood, guidance grammars just compile down to a [lark-like syntax for describing context free grammars](https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md).\n",
    "\n",
    "We can create a tool using this lark syntax directly. Note that the same caveats about arguments to the callable still apply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be0b540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'calculator',\n",
       " 'description': 'A calculator that can evaluate mathematical expressions.',\n",
       " 'tool': {'type': 'custom',\n",
       "  'format': {'type': 'grammar',\n",
       "   'syntax': 'lark',\n",
       "   'definition': '\\nstart: expression\\n\\nexpression: term\\n     | expression \"+\" term\\n     | expression \"-\" term\\n\\nterm: factor\\n     | term \"*\" factor\\n     | term \"/\" factor\\n\\nfactor: NUMBER\\n     | \"(\" expression \")\"\\n\\nNUMBER: /\\\\d+(\\\\.\\\\d+)?/\\n'}},\n",
       " 'callable': <function __main__.evaluate_expression(expr: str) -> Union[int, float]>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lark = r\"\"\"\n",
    "start: expression\n",
    "\n",
    "expression: term\n",
    "     | expression \"+\" term\n",
    "     | expression \"-\" term\n",
    "\n",
    "term: factor\n",
    "     | term \"*\" factor\n",
    "     | term \"/\" factor\n",
    "\n",
    "factor: NUMBER\n",
    "     | \"(\" expression \")\"\n",
    "\n",
    "NUMBER: /\\d+(\\.\\d+)?/\n",
    "\"\"\"\n",
    "\n",
    "calculator_tool = Tool.from_lark(\n",
    "    lark=lark,\n",
    "    name=\"calculator\",\n",
    "    description=\"A calculator that can evaluate mathematical expressions.\",\n",
    "    callable=evaluate_expression\n",
    ")\n",
    "calculator_tool.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78a7e1",
   "metadata": {},
   "source": [
    "# Calling Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e643e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    get_weather_tool,\n",
    "    whois_tool,\n",
    "    calculator_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c68755",
   "metadata": {},
   "source": [
    "Most remote models have native support for tool-calling with function (JSON-specified) tools.\n",
    "\n",
    "With OpenAI's release of GPT-5, they additionally support tools that are specified by regex or lark-style context free grammars. In these cases, model sampling is constrained using [LLGuidance](https://\n",
    "github.com/guidance-ai/llguidance/blob/main/docs/syntax.md)\n",
    "\n",
    "To allow our LM to use our tools, we simply pass them in a list as an argument to `gen`. If the LM wants to call one of our tools, `guidance` will handle the callback for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbfeb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c0aad213594666ac70bdcf5da0f0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance.models import OpenAI\n",
    "from guidance import user, assistant, gen\n",
    "lm = OpenAI(\"gpt-5-mini\")\n",
    "\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=tools, tool_choice=\"required\")\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75554733",
   "metadata": {},
   "source": [
    "You may notice that the tool call is wrapped in `<function={name}>...</function>` tags and the result is wrapped in `<function_result>..</function_result>` tags. This is purely for visualization purposes, as OpenAI uses a structured representation under the hood which we (somewhat arbitrarily) represent as a string above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94270de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473c09ce0a6546848bef11728e41019f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = OpenAI(\"gpt-5-mini\")\n",
    "\n",
    "with user():\n",
    "    lm += \"If Johnny has 5 apples and gives 2 to Mary, how many apples does he have left?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=tools, tool_choice=\"required\")\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "387d3cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bc4d8f4b7f4bba9f95fd7bd1f49cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = OpenAI(\"gpt-5-mini\")\n",
    "\n",
    "with user():\n",
    "    lm += \"Who owns the openai website?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=tools, tool_choice=\"required\")\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ca5dd",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea09aed",
   "metadata": {},
   "source": [
    "We first define our tool-calling syntax in a system prompt.\n",
    "\n",
    "If your model has been fine-tuned to follow a specific format, you may want to ensure that your definitions here are in-line with that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db2c66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "\n",
    "system_prompt = f\"\"\"\\\n",
    "# Tool Instructions\n",
    "You have access to the following functions:\n",
    "{\n",
    "    dumps(\n",
    "        [\n",
    "            {\"name\": tool.name, \"description\": tool.description, \"arguments\": tool.schema_dump()}\n",
    "            for tool in tools\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "If you choose to call a function, you must return the function name and arguments in the following format:\n",
    "<function={{function_name}}>{{parameters}}</function>\n",
    "where {{function_name}} is the name of the function you are calling, and {{parameters}} is a JSON object containing the parameters for the function call.\n",
    "\n",
    "The return value of the function call will be provided to you in the following format:\n",
    "<function_result>{{return_value}}</function_result>\n",
    "\n",
    "After the return value is provided, you should summarize the information and provide a final response.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b189adc",
   "metadata": {},
   "source": [
    "Other imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d9de977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, system, user, assistant, gen\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-3B-Instruct-Q6_K_L.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ff0fa",
   "metadata": {},
   "source": [
    "## The hard way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502ba9a",
   "metadata": {},
   "source": [
    "We now need to construct a grammar that enforces that the model behaves in-line with the system prompt that we wrote.\n",
    "\n",
    "We first construct a grammar that ensures that any time the model produces the string `<function`, it is *forced* to\n",
    "1. follow that with a valid function name\n",
    "2. then follow the correct function's arguments\n",
    "\n",
    "Here we will build this grammar directly using `llguidance`'s `lark`-like BNF syntax ([see documentation here](https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md#tool-calling))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e0ce1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Tool grammar:\n",
       "```lark\n",
       "start: TEXT | function_call\n",
       "function_trg[lazy]: TEXT \"<function=\"\n",
       "function_end: \"</function>\"\n",
       "TEXT: /(.|\\n)*/\n",
       "function_call_get_weather: function_get_weather_begin function_get_weather_args function_end\n",
       "function_get_weather_begin: function_trg \"get_weather>\"\n",
       "function_get_weather_args: %json {\"additionalProperties\": false, \"properties\": {\"city\": {\"title\": \"City\", \"type\": \"string\"}, \"unit\": {\"enum\": [\"celsius\", \"fahrenheit\"], \"title\": \"Unit\", \"type\": \"string\"}}, \"required\": [\"city\", \"unit\"], \"title\": \"get_weather\", \"type\": \"object\"}\n",
       "function_call[capture]: function_call_get_weather\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "tool_grammar_raw = r\"\"\"start: TEXT | function_call\n",
    "function_trg[lazy]: TEXT \"<function=\"\n",
    "function_end: \"</function>\"\n",
    "TEXT: /(.|\\n)*/\"\"\"\n",
    "\n",
    "for tool in tools:\n",
    "    tool_grammar_raw += \"\"\"\n",
    "function_call_{name}: function_{name}_begin function_{name}_args function_end\n",
    "function_{name}_begin: function_trg \"{name}>\"\n",
    "function_{name}_args: %json {schema}\"\"\".format(\n",
    "    name=tool.name,\n",
    "    schema=dumps(tool.schema_dump())\n",
    ")\n",
    "\n",
    "tool_grammar_raw += f\"\"\"\n",
    "function_call[capture]: {\" | \".join(\"function_call_\" + tool.name for tool in tools)}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(f\"Tool grammar:\\n```lark\\n{tool_grammar_raw}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd23c9",
   "metadata": {},
   "source": [
    "### Running the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af4b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991709db7ab14029b84c7f661ceb3efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance import lark\n",
    "\n",
    "# Create the grammar from the raw string\n",
    "tool_grammar = lark(tool_grammar_raw)\n",
    "# Just double-checking that we wrote that correctly\n",
    "tool_grammar._llguidance_validate()\n",
    "\n",
    "lm = models.LlamaCpp(\n",
    "    model_path,\n",
    "    n_ctx=4096,\n",
    ")\n",
    "\n",
    "with system():\n",
    "    lm += system_prompt\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += tool_grammar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d855a",
   "metadata": {},
   "source": [
    "### Calling the function\n",
    "Note that in the `llguidance` grammar, we captured the function call with the name \"function_call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28e76b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=get_weather>{\"city\": \"San Francisco\", \"unit\": \"celsius\"}</function>\n"
     ]
    }
   ],
   "source": [
    "print(lm[\"function_call\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a73a4348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: get_weather\n",
      "Function arguments: {\"city\": \"San Francisco\", \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.match(\n",
    "    r\"<function=(?P<name>[^>]+)>(?P<args>\\{(.|\\n)*\\})</function>\",\n",
    "    lm[\"function_call\"]\n",
    ")\n",
    "name = match.group(\"name\")\n",
    "args = match.group(\"args\")\n",
    "print(f\"Function name: {name}\")\n",
    "print(f\"Function arguments: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59e8d66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function return value: The current temperature in San Francisco is 19 degrees celsius and it is cloudy.\n"
     ]
    }
   ],
   "source": [
    "matching_functions = [\n",
    "    tool for tool in tools if tool.name == name\n",
    "]\n",
    "assert len(matching_functions) == 1, \"Function name not found in tools\"\n",
    "function = matching_functions[0]\n",
    "function_args = function.validate_args(args)\n",
    "return_value = function.callable(**function_args)\n",
    "\n",
    "print(f\"Function return value: {return_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0d907",
   "metadata": {},
   "source": [
    "We have to add the function's return value to the model's context, as we promised.\n",
    "\n",
    "Then we can finally let the model summarize the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22b3140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45fd3e8a0084893897a9cf775f47c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with user():\n",
    "    lm += f\"<function_result>{return_value}</function_result>\"\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2940f82",
   "metadata": {},
   "source": [
    "## The easy way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0800d2d",
   "metadata": {},
   "source": [
    "We can subclass `guidance.ToolCallHandler` in order to encapsulate some of the logic we had to manually write out above.\n",
    "\n",
    "See the `CustomHandler` class defined below -- the methods you see defined here must be implemented by all subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129678e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import ToolCallHandler, json as gen_json, gen\n",
    "from guidance.types import Grammar\n",
    "from typing import Any\n",
    "import re\n",
    "from json import dumps, loads\n",
    "\n",
    "\n",
    "class CustomHandler(ToolCallHandler):\n",
    "    def trigger(self):\n",
    "        \"\"\"\n",
    "        A substring or token that signals when to start applying a tool.\n",
    "        For example \"<function\" or \"<|python_tag|>\".\n",
    "        \"\"\"\n",
    "        return \"<function\"\n",
    "\n",
    "    def begin(self, tool_name: str) -> str:\n",
    "        \"\"\"\n",
    "        The beginning of the tool call. Must start with trigger. May depend on the tool name.\n",
    "        For example \"<function=foo>\", '<|python_tag|>{\"name\":\"foo\",\"parameters\":', or just \"<|python_tag|>\".\n",
    "        \"\"\"\n",
    "        return f\"<function={tool_name}>\"\n",
    "\n",
    "    def body(self, tool: Tool) -> Grammar:\n",
    "        \"\"\"\n",
    "        The body of the tool call. Should return a GrammarNode that matches the tool call arguments.\n",
    "        For example, if begin contains the tool name, it may be given by\n",
    "        ```python\n",
    "            guidance.json(schema=tool.args.model_json_schema())\n",
    "        ```\n",
    "        or if begin does not contain the tool name, it may be given by\n",
    "        ```python\n",
    "        guidance.json(\n",
    "            schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"parameters\": tool.args.model_json_schema(),\n",
    "                },\n",
    "                \"required\": [\"name\", \"parameters\"],\n",
    "                \"additionalProperties\": False,\n",
    "            }\n",
    "        )\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return gen_json(schema=tool.schema.model_json_schema())\n",
    "\n",
    "    def end(self) -> str:\n",
    "        \"\"\"\n",
    "        The end of the tool call. Should return a string that ends the tool call.\n",
    "        For example \"</function><|eot_id|>\\n\" or \"<|python_tag|><eom_id>\".\n",
    "        \"\"\"\n",
    "        return \"</function>\\n\"\n",
    "\n",
    "    def parse_tool_calls(self, text: str) -> tuple[str, dict[str, Any]]: \n",
    "        \"\"\"\n",
    "        Parse the tool calls from the text.\n",
    "        Should return a list of tuples with tool name and args.\n",
    "        \"\"\"\n",
    "        matches = re.finditer(\n",
    "            r\"<function=(?P<name>[^>]+)>(?P<args>\\{(.|\\n)*\\})</function>\",\n",
    "            text\n",
    "        )\n",
    "        tool_calls = []\n",
    "        for match in matches:\n",
    "            tool_calls.append((match.group(\"name\"), loads(match.group(\"args\"))))\n",
    "        return tool_calls\n",
    "\n",
    "    def format_return_value(self, value: Any) -> str:\n",
    "        \"\"\"\n",
    "        Format the return value of the tool call.\n",
    "        Should return a string representation of the value.\n",
    "        \"\"\"\n",
    "        return f\"<function_result>{dumps(value)}</function_result>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac1848",
   "metadata": {},
   "source": [
    "If we pass the tool call handler to our model init, we can then follow the same simple tool-calling pattern we used for remote models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce967c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03860810cda48378e2d4106127461c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = models.LlamaCpp(\n",
    "    model_path,\n",
    "    n_ctx=4096,\n",
    "    tool_call_handler_cls=CustomHandler\n",
    ")\n",
    "\n",
    "with system():\n",
    "    lm += system_prompt\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=tools)\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance-ai (3.10.15)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
