{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213de727",
   "metadata": {},
   "source": [
    "# Defining Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import Tool\n",
    "from typing import Literal\n",
    "import random\n",
    "\n",
    "def get_weather(city: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given city.\n",
    "    \n",
    "    Args:\n",
    "        city (str): The name of the city to get the weather for.\n",
    "        unit (Literal[\"celsius\", \"fahrenheit\"]): The unit of temperature to return.\n",
    "    \n",
    "    Returns: \n",
    "        str: A string describing the current weather in the specified city.\n",
    "    \"\"\"\n",
    "    temp = random.randint(-10, 35)\n",
    "    if temp <= 0:\n",
    "        weather = \"snowy\"\n",
    "    elif temp <= 20:\n",
    "        weather = \"cloudy\"\n",
    "    else:\n",
    "        weather = \"sunny\"\n",
    "    if unit == \"fahrenheit\":\n",
    "        temp = temp * 9/5 + 32\n",
    "    return f\"The current temperature in {city} is {temp} degrees {unit} and it is {weather}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02b0f6",
   "metadata": {},
   "source": [
    "A `Tool` consist of:\n",
    "1. A `callable` that should be invoked when the tool is called\n",
    "2. A `name` that uniquely identifies the callable\n",
    "3. A `description` that indicates the usage of the callable\n",
    "4. A `schema` that specifies the types of the callable's arguments, along with any metadata such as examples, default values, etc.\n",
    "    - This `schema` may be a JSON schema (`dict[str, Any]`) or a `pydantic.BaseModel`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83894ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name:\n",
      "get_weather\n",
      "\n",
      "Tool description:\n",
      "Get the current weather for a given city.\n",
      "\n",
      "Tool schema:\n",
      "{'additionalProperties': False, 'properties': {'city': {'description': 'The name of the city to get the weather for.', 'title': 'City', 'type': 'string'}, 'unit': {'default': 'celsius', 'description': 'The unit of temperature to return.', 'enum': ['celsius', 'fahrenheit'], 'title': 'Unit', 'type': 'string'}}, 'required': ['city'], 'title': 'GetWeatherArgs', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"The name of the city to get the weather for.\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(\"celsius\", description=\"The unit of temperature to return.\")\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra=\"forbid\", # Disallow extra fields -- required for OpenAI\n",
    "    )\n",
    "\n",
    "tool = Tool(\n",
    "    callable=get_weather,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a given city.\",\n",
    "    schema=GetWeatherArgs\n",
    ")\n",
    "\n",
    "print(f\"\"\"\\\n",
    "Tool name:\n",
    "{tool.name}\n",
    "\n",
    "Tool description:\n",
    "{tool.description}\n",
    "\n",
    "Tool schema:\n",
    "{tool.schema_dump()}\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8ae54",
   "metadata": {},
   "source": [
    "As an alternative to constructing a `Tool` manually, `guidance` can infer the tool's `name`, `description`, and `schema` from the callable's name, docstring, and signature annotations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b34db569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name:\n",
      "get_weather\n",
      "\n",
      "Tool description:\n",
      "Get the current weather for a given city.\n",
      "    \n",
      "    Args:\n",
      "        city (str): The name of the city to get the weather for.\n",
      "        unit (Literal[\"celsius\", \"fahrenheit\"]): The unit of temperature to return.\n",
      "    \n",
      "    Returns: \n",
      "        str: A string describing the current weather in the specified city.\n",
      "\n",
      "Tool schema:\n",
      "{'additionalProperties': False, 'properties': {'city': {'title': 'City', 'type': 'string'}, 'unit': {'enum': ['celsius', 'fahrenheit'], 'title': 'Unit', 'type': 'string'}}, 'required': ['city', 'unit'], 'title': 'get_weather', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "tool = Tool.from_callable(get_weather)\n",
    "\n",
    "print(f\"\"\"\\\n",
    "Tool name:\n",
    "{tool.name}\n",
    "\n",
    "Tool description:\n",
    "{tool.description}\n",
    "\n",
    "Tool schema:\n",
    "{tool.schema_dump()}\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78a7e1",
   "metadata": {},
   "source": [
    "# Calling Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063b48c",
   "metadata": {},
   "source": [
    "## Remote Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c68755",
   "metadata": {},
   "source": [
    "Most remote models have native support for tool-calling.\n",
    "\n",
    "With these models, you can simply pass a list of tools to `gen`.\n",
    "\n",
    "Behind the scenes, `guidance` automatically translates the `gen` to a call to the model provider's API, which in the case of OpenAI looks like:\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI(...)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[...],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.schema.model_json_schema(),\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "*Note*: you may pass a list of `callable` instead, and the `Tool`s will be automatically constructed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c9654415914857a8005fc21cfbcf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance.models import OpenAI\n",
    "from guidance import user, assistant, gen\n",
    "lm = OpenAI(\"gpt-4o-mini\")\n",
    "\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=[tool])\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b9ccf",
   "metadata": {},
   "source": [
    "You may notice that the tool call is wrapped in `<function={name}>...</function>` tags and the result is wrapped in `<function_result>..</function_result>` tags. This is purely for visualization purposes, as OpenAI uses a structured representation under the hood which we (somewhat arbitrarily) represent as a string above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ca5dd",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129678e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import ToolCallHandler, json as gen_json\n",
    "from guidance.types import Grammar\n",
    "from typing import Any\n",
    "import re\n",
    "from json import dumps, loads\n",
    "\n",
    "\n",
    "class CustomHandler(ToolCallHandler):\n",
    "    def trigger(self):\n",
    "        return \"<function\"\n",
    "\n",
    "    def begin(self, tool_name: str) -> str:\n",
    "        return f\"<function={tool_name}>\"\n",
    "\n",
    "    def body(self, tool: Tool) -> Grammar:\n",
    "        return gen_json(schema=tool.schema.model_json_schema())\n",
    "\n",
    "    def end(self) -> str:\n",
    "        return \"</function>\\n\"\n",
    "\n",
    "    def parse_tool_calls(self, text: str) -> tuple[str, dict[str, Any]]:\n",
    "        matches = re.finditer(\n",
    "            r\"<function=(?P<name>[^>]+)>(?P<args>\\{(.|\\n)*\\})</function>\",\n",
    "            text\n",
    "        )\n",
    "        tool_calls = []\n",
    "        for match in matches:\n",
    "            tool_calls.append((match.group(\"name\"), loads(match.group(\"args\"))))\n",
    "        return tool_calls\n",
    "\n",
    "    def format_return_value(self, value: Any) -> str:\n",
    "        return f\"<function_result>{dumps(value)}</function_result>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce967c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e418b5fd64764db3aa0964186c7cdbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance.models import LlamaCpp\n",
    "from guidance import system, user, assistant, gen\n",
    "from huggingface_hub import hf_hub_download\n",
    "from json import dumps\n",
    "\n",
    "lm = LlamaCpp(\n",
    "    hf_hub_download(\n",
    "        repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "        filename=\"Llama-3.2-3B-Instruct-Q6_K_L.gguf\",\n",
    "    ),\n",
    "    n_ctx=4096,\n",
    "    tool_call_handler_cls=CustomHandler\n",
    ")\n",
    "\n",
    "with system():\n",
    "    lm += f\"\"\"\\\n",
    "# Tool Instructions\n",
    "You have access to the following functions:\n",
    "{\n",
    "    dumps(\n",
    "        [\n",
    "            {\"name\": tool.name, \"description\": tool.description, \"arguments\": tool.schema_dump()}\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "If you choose to call a function, you must return the function name and arguments in the following format:\n",
    "<function={{function_name}}>{{parameters}}</function>\n",
    "where {{function_name}} is the name of the function you are calling, and {{parameters}} is a JSON object containing the parameters for the function call.\n",
    "\n",
    "The return value of the function call will be provided to you in the following format:\n",
    "<function_result>{{return_value}}</function_result>\n",
    "\n",
    "After the return value is provided, you should summarize the information and provide a final response.\n",
    "\"\"\"\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm  += gen(tools=[tool]) \n",
    "with assistant():\n",
    "    lm  += gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1a3b327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024# Tool Instructions\n",
      "You have access to the following functions:\n",
      "[{\"name\": \"get_weather\", \"description\": \"Get the current weather for a given city.\\n    \\n    Args:\\n        city (str): The name of the city to get the weather for.\\n        unit (Literal[\\\"celsius\\\", \\\"fahrenheit\\\"]): The unit of temperature to return.\\n    \\n    Returns: \\n        str: A string describing the current weather in the specified city.\", \"arguments\": {\"additionalProperties\": false, \"properties\": {\"city\": {\"title\": \"City\", \"type\": \"string\"}, \"unit\": {\"enum\": [\"celsius\", \"fahrenheit\"], \"title\": \"Unit\", \"type\": \"string\"}}, \"required\": [\"city\", \"unit\"], \"title\": \"get_weather\", \"type\": \"object\"}}]\n",
      "\n",
      "If you choose to call a function, you must return the function name and arguments in the following format:\n",
      "<function={function_name}>{parameters}</function>\n",
      "where {function_name} is the name of the function you are calling, and {parameters} is a JSON object containing the parameters for the function call.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the weather like in San Francisco?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<function=get_weather>{\"city\": \"San Francisco\", \"unit\": \"celsius\"}</function><|eot_id|>\n",
      "<|start_header_id|>ipython<|end_header_id|>\n",
      "\"The current temperature in San Francisco is 33 degrees celsius and it is raining.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "It seems like the function call I provided earlier didn't return the expected result. Let me try again.\n",
      "\n",
      "<function=get_weather>{\"city\": \"San Francisco\", \"unit\": \"fahrenheit\"}</function>\n",
      "\n",
      "Please note that I've changed the unit to \"fahrenheit\" as the initial call was in \"celsius\".\n"
     ]
    }
   ],
   "source": [
    "print(str(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f0085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance-ai (3.10.15)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
