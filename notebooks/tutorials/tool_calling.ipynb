{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213de727",
   "metadata": {},
   "source": [
    "# Defining Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import Tool\n",
    "from typing import Literal\n",
    "import random\n",
    "\n",
    "def get_weather(city: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given city.\n",
    "    \n",
    "    Args:\n",
    "        city (str): The name of the city to get the weather for.\n",
    "        unit (Literal[\"celsius\", \"fahrenheit\"]): The unit of temperature to return.\n",
    "    \n",
    "    Returns: \n",
    "        str: A string describing the current weather in the specified city.\n",
    "    \"\"\"\n",
    "    temp = random.randint(-10, 35)\n",
    "    if temp <= 0:\n",
    "        weather = \"snowy\"\n",
    "    elif temp <= 20:\n",
    "        weather = \"cloudy\"\n",
    "    else:\n",
    "        weather = \"sunny\"\n",
    "    if unit == \"fahrenheit\":\n",
    "        temp = temp * 9/5 + 32\n",
    "    return f\"The current temperature in {city} is {temp} degrees {unit} and it is {weather}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02b0f6",
   "metadata": {},
   "source": [
    "A `Tool` consist of:\n",
    "1. A `callable` that should be invoked when the tool is called\n",
    "2. A `name` that uniquely identifies the callable\n",
    "3. A `description` that indicates the usage of the callable\n",
    "4. A `schema` that specifies the types of the callable's arguments, along with any metadata such as examples, default values, etc.\n",
    "    - This `schema` may be a JSON schema (`dict[str, Any]`) or a `pydantic.BaseModel`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83894ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name:\n",
      "get_weather\n",
      "\n",
      "Tool description:\n",
      "Get the current weather for a given city.\n",
      "\n",
      "Tool schema:\n",
      "{'additionalProperties': False, 'properties': {'city': {'description': 'The name of the city to get the weather for.', 'title': 'City', 'type': 'string'}, 'unit': {'default': 'celsius', 'description': 'The unit of temperature to return.', 'enum': ['celsius', 'fahrenheit'], 'title': 'Unit', 'type': 'string'}}, 'required': ['city'], 'title': 'GetWeatherArgs', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"The name of the city to get the weather for.\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(\"celsius\", description=\"The unit of temperature to return.\")\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra=\"forbid\", # Disallow extra fields -- required for OpenAI\n",
    "    )\n",
    "\n",
    "tool = Tool(\n",
    "    callable=get_weather,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a given city.\",\n",
    "    schema=GetWeatherArgs\n",
    ")\n",
    "\n",
    "print(f\"\"\"\\\n",
    "Tool name:\n",
    "{tool.name}\n",
    "\n",
    "Tool description:\n",
    "{tool.description}\n",
    "\n",
    "Tool schema:\n",
    "{tool.schema_dump()}\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8ae54",
   "metadata": {},
   "source": [
    "As an alternative to constructing a `Tool` manually, `guidance` can infer the tool's `name`, `description`, and `schema` from the callable's name, docstring, and signature annotations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b34db569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name:\n",
      "get_weather\n",
      "\n",
      "Tool description:\n",
      "Get the current weather for a given city.\n",
      "    \n",
      "    Args:\n",
      "        city (str): The name of the city to get the weather for.\n",
      "        unit (Literal[\"celsius\", \"fahrenheit\"]): The unit of temperature to return.\n",
      "    \n",
      "    Returns: \n",
      "        str: A string describing the current weather in the specified city.\n",
      "\n",
      "Tool schema:\n",
      "{'additionalProperties': False, 'properties': {'city': {'title': 'City', 'type': 'string'}, 'unit': {'enum': ['celsius', 'fahrenheit'], 'title': 'Unit', 'type': 'string'}}, 'required': ['city', 'unit'], 'title': 'get_weather', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "tool = Tool.from_callable(get_weather)\n",
    "\n",
    "print(f\"\"\"\\\n",
    "Tool name:\n",
    "{tool.name}\n",
    "\n",
    "Tool description:\n",
    "{tool.description}\n",
    "\n",
    "Tool schema:\n",
    "{tool.schema_dump()}\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78a7e1",
   "metadata": {},
   "source": [
    "# Calling Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25e643e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool.from_callable(get_weather)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063b48c",
   "metadata": {},
   "source": [
    "## Remote Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c68755",
   "metadata": {},
   "source": [
    "Most remote models have native support for tool-calling.\n",
    "\n",
    "With these models, you can simply pass a list of tools to `gen`.\n",
    "\n",
    "Behind the scenes, `guidance` automatically translates the `gen` to a call to the model provider's API, which in the case of OpenAI looks like:\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI(...)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[...],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.schema.model_json_schema(),\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "*Note*: you may pass a list of `callable` instead, and the `Tool`s will be automatically constructed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c9654415914857a8005fc21cfbcf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance.models import OpenAI\n",
    "from guidance import user, assistant, gen\n",
    "lm = OpenAI(\"gpt-4o-mini\")\n",
    "\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=tools)\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b9ccf",
   "metadata": {},
   "source": [
    "You may notice that the tool call is wrapped in `<function={name}>...</function>` tags and the result is wrapped in `<function_result>..</function_result>` tags. This is purely for visualization purposes, as OpenAI uses a structured representation under the hood which we (somewhat arbitrarily) represent as a string above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ca5dd",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea09aed",
   "metadata": {},
   "source": [
    "We first define our tool-calling syntax in a system prompt.\n",
    "\n",
    "If your model has been fine-tuned to follow a specific format, you may want to ensure that your definitions here are in-line with that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db2c66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "\n",
    "system_prompt = f\"\"\"\\\n",
    "# Tool Instructions\n",
    "You have access to the following functions:\n",
    "{\n",
    "    dumps(\n",
    "        [\n",
    "            {\"name\": tool.name, \"description\": tool.description, \"arguments\": tool.schema_dump()}\n",
    "            for tool in tools\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "If you choose to call a function, you must return the function name and arguments in the following format:\n",
    "<function={{function_name}}>{{parameters}}</function>\n",
    "where {{function_name}} is the name of the function you are calling, and {{parameters}} is a JSON object containing the parameters for the function call.\n",
    "\n",
    "The return value of the function call will be provided to you in the following format:\n",
    "<function_result>{{return_value}}</function_result>\n",
    "\n",
    "After the return value is provided, you should summarize the information and provide a final response.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b189adc",
   "metadata": {},
   "source": [
    "Other imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d9de977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, system, user, assistant, gen\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-3B-Instruct-Q6_K_L.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ff0fa",
   "metadata": {},
   "source": [
    "## The hard way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502ba9a",
   "metadata": {},
   "source": [
    "We now need to construct a grammar that enforces that the model behaves in-line with the system prompt that we wrote.\n",
    "\n",
    "We first construct a grammar that ensures that any time the model produces the string `<function`, it is *forced* to\n",
    "1. follow that with a valid function name\n",
    "2. then follow the correct function's arguments\n",
    "\n",
    "Here we will build this grammar directly using `llguidance`'s `lark`-like BNF syntax ([see documentation here](https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md#tool-calling))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e0ce1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Tool grammar:\n",
       "```lark\n",
       "start: TEXT | function_call\n",
       "function_trg[lazy]: TEXT \"<function=\"\n",
       "function_end: \"</function>\"\n",
       "TEXT: /(.|\\n)*/\n",
       "function_call_get_weather: function_get_weather_begin function_get_weather_args function_end\n",
       "function_get_weather_begin: function_trg \"get_weather>\"\n",
       "function_get_weather_args: %json {\"additionalProperties\": false, \"properties\": {\"city\": {\"title\": \"City\", \"type\": \"string\"}, \"unit\": {\"enum\": [\"celsius\", \"fahrenheit\"], \"title\": \"Unit\", \"type\": \"string\"}}, \"required\": [\"city\", \"unit\"], \"title\": \"get_weather\", \"type\": \"object\"}\n",
       "function_call[capture]: function_call_get_weather\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "tool_grammar_raw = r\"\"\"start: TEXT | function_call\n",
    "function_trg[lazy]: TEXT \"<function=\"\n",
    "function_end: \"</function>\"\n",
    "TEXT: /(.|\\n)*/\"\"\"\n",
    "\n",
    "for tool in tools:\n",
    "    tool_grammar_raw += \"\"\"\n",
    "function_call_{name}: function_{name}_begin function_{name}_args function_end\n",
    "function_{name}_begin: function_trg \"{name}>\"\n",
    "function_{name}_args: %json {schema}\"\"\".format(\n",
    "    name=tool.name,\n",
    "    schema=dumps(tool.schema_dump())\n",
    ")\n",
    "\n",
    "tool_grammar_raw += f\"\"\"\n",
    "function_call[capture]: {\" | \".join(\"function_call_\" + tool.name for tool in tools)}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(f\"Tool grammar:\\n```lark\\n{tool_grammar_raw}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd23c9",
   "metadata": {},
   "source": [
    "### Running the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af4b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991709db7ab14029b84c7f661ceb3efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance import lark\n",
    "\n",
    "# Create the grammar from the raw string\n",
    "tool_grammar = lark(tool_grammar_raw)\n",
    "# Just double-checking that we wrote that correctly\n",
    "tool_grammar._llguidance_validate()\n",
    "\n",
    "lm = models.LlamaCpp(\n",
    "    model_path,\n",
    "    n_ctx=4096,\n",
    ")\n",
    "\n",
    "with system():\n",
    "    lm += system_prompt\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += tool_grammar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d855a",
   "metadata": {},
   "source": [
    "### Calling the function\n",
    "Note that in the `llguidance` grammar, we captured the function call with the name \"function_call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28e76b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=get_weather>{\"city\": \"San Francisco\", \"unit\": \"celsius\"}</function>\n"
     ]
    }
   ],
   "source": [
    "print(lm[\"function_call\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a73a4348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: get_weather\n",
      "Function arguments: {\"city\": \"San Francisco\", \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.match(\n",
    "    r\"<function=(?P<name>[^>]+)>(?P<args>\\{(.|\\n)*\\})</function>\",\n",
    "    lm[\"function_call\"]\n",
    ")\n",
    "name = match.group(\"name\")\n",
    "args = match.group(\"args\")\n",
    "print(f\"Function name: {name}\")\n",
    "print(f\"Function arguments: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59e8d66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function return value: The current temperature in San Francisco is 19 degrees celsius and it is cloudy.\n"
     ]
    }
   ],
   "source": [
    "matching_functions = [\n",
    "    tool for tool in tools if tool.name == name\n",
    "]\n",
    "assert len(matching_functions) == 1, \"Function name not found in tools\"\n",
    "function = matching_functions[0]\n",
    "function_args = function.validate_args(args)\n",
    "return_value = function.callable(**function_args)\n",
    "\n",
    "print(f\"Function return value: {return_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0d907",
   "metadata": {},
   "source": [
    "We have to add the function's return value to the model's context, as we promised.\n",
    "\n",
    "Then we can finally let the model summarize the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22b3140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45fd3e8a0084893897a9cf775f47c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with user():\n",
    "    lm += f\"<function_result>{return_value}</function_result>\"\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2940f82",
   "metadata": {},
   "source": [
    "## The easy way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0800d2d",
   "metadata": {},
   "source": [
    "We can subclass `guidance.ToolCallHandler` in order to encapsulate some of the logic we had to manually write out above.\n",
    "\n",
    "See the `CustomHandler` class defined below -- the methods you see defined here must be implemented by all subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129678e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import ToolCallHandler, json as gen_json, gen\n",
    "from guidance.types import Grammar\n",
    "from typing import Any\n",
    "import re\n",
    "from json import dumps, loads\n",
    "\n",
    "\n",
    "class CustomHandler(ToolCallHandler):\n",
    "    def trigger(self):\n",
    "        \"\"\"\n",
    "        A substring or token that signals when to start applying a tool.\n",
    "        For example \"<function\" or \"<|python_tag|>\".\n",
    "        \"\"\"\n",
    "        return \"<function\"\n",
    "\n",
    "    def begin(self, tool_name: str) -> str:\n",
    "        \"\"\"\n",
    "        The beginning of the tool call. Must start with trigger. May depend on the tool name.\n",
    "        For example \"<function=foo>\", '<|python_tag|>{\"name\":\"foo\",\"parameters\":', or just \"<|python_tag|>\".\n",
    "        \"\"\"\n",
    "        return f\"<function={tool_name}>\"\n",
    "\n",
    "    def body(self, tool: Tool) -> Grammar:\n",
    "        \"\"\"\n",
    "        The body of the tool call. Should return a GrammarNode that matches the tool call arguments.\n",
    "        For example, if begin contains the tool name, it may be given by\n",
    "        ```python\n",
    "            guidance.json(schema=tool.args.model_json_schema())\n",
    "        ```\n",
    "        or if begin does not contain the tool name, it may be given by\n",
    "        ```python\n",
    "        guidance.json(\n",
    "            schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"parameters\": tool.args.model_json_schema(),\n",
    "                },\n",
    "                \"required\": [\"name\", \"parameters\"],\n",
    "                \"additionalProperties\": False,\n",
    "            }\n",
    "        )\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return gen_json(schema=tool.schema.model_json_schema())\n",
    "\n",
    "    def end(self) -> str:\n",
    "        \"\"\"\n",
    "        The end of the tool call. Should return a string that ends the tool call.\n",
    "        For example \"</function><|eot_id|>\\n\" or \"<|python_tag|><eom_id>\".\n",
    "        \"\"\"\n",
    "        return \"</function>\\n\"\n",
    "\n",
    "    def parse_tool_calls(self, text: str) -> tuple[str, dict[str, Any]]: \n",
    "        \"\"\"\n",
    "        Parse the tool calls from the text.\n",
    "        Should return a list of tuples with tool name and args.\n",
    "        \"\"\"\n",
    "        matches = re.finditer(\n",
    "            r\"<function=(?P<name>[^>]+)>(?P<args>\\{(.|\\n)*\\})</function>\",\n",
    "            text\n",
    "        )\n",
    "        tool_calls = []\n",
    "        for match in matches:\n",
    "            tool_calls.append((match.group(\"name\"), loads(match.group(\"args\"))))\n",
    "        return tool_calls\n",
    "\n",
    "    def format_return_value(self, value: Any) -> str:\n",
    "        \"\"\"\n",
    "        Format the return value of the tool call.\n",
    "        Should return a string representation of the value.\n",
    "        \"\"\"\n",
    "        return f\"<function_result>{dumps(value)}</function_result>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac1848",
   "metadata": {},
   "source": [
    "If we pass the tool call handler to our model init, we can then follow the same simple tool-calling pattern we used for remote models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce967c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03860810cda48378e2d4106127461c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = models.LlamaCpp(\n",
    "    model_path,\n",
    "    n_ctx=4096,\n",
    "    tool_call_handler_cls=CustomHandler\n",
    ")\n",
    "\n",
    "with system():\n",
    "    lm += system_prompt\n",
    "with user():\n",
    "    lm += \"What is the weather like in San Francisco?\"\n",
    "with assistant():\n",
    "    lm += gen(tools=tools)\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance-ai (3.10.15)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
