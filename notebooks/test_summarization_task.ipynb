{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../key.txt', 'r') as f:\n",
    "    key_file = f.read()\n",
    "\n",
    "key_file = key_file.split('\\n')\n",
    "keys = {}\n",
    "for key in key_file:\n",
    "    key = key.split(':')\n",
    "    keys[key[0]] =  key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test between the Summarization Task by Langchain & Guidance\n",
    "- Detail:\n",
    "    - Objects: test the performance of summarization between Langchain & Guidance\n",
    "    - BenchMark: Runtime\n",
    "    - LLM: OpenAI API - \"text-davinci-003\"\n",
    "    - Method: Map-Reduce\n",
    "        1. Split text into chunks\n",
    "        2. Do summarization on each chunks and get summaries\n",
    "        3. Do final summarization on the summaries\n",
    "\n",
    "- Langchain:\n",
    "    - using map_reduce_chain\n",
    "\n",
    "- Guidance:\n",
    "    - using {{#each}}\n",
    "    - using {{#geneach}}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- mock web text\n",
    "- text splitting & processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Web Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_content = \"\"\"\n",
    "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
    "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
    "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
    "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
    "Several AI content writing tools currently use GPT-3, such as:\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\n",
    "BERT serves as the base for a number of services, like:\n",
    "The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\n",
    "Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\n",
    "Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\n",
    "Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\n",
    "They use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\n",
    "They are unsupervised learning models (they don’t require labeled data for training).\n",
    "They can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\n",
    "Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\n",
    "However, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\n",
    "On other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\n",
    "The bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\n",
    "For example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\n",
    "\"\"\"\n",
    "\n",
    "text2_content = \"\"\"\n",
    "Stack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\n",
    "                                        Learn more about Stack Overflow the company, and our products.\n",
    "Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\n",
    "In the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\n",
    "In the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\n",
    "I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\n",
    "Q1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\n",
    "Q2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\n",
    "Q4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\n",
    "To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\n",
    "BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\n",
    "Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\n",
    "Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\n",
    "The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\n",
    "By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
    "Not the answer you're looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
    "YA novel about a girl in dystopian society who gets surgically altered to be a double for a dead girl\n",
    "Is lying in an application for a job at a private company, signed under penalty of perjury, prosecutable as perjury?\n",
    "DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\". Something new or based on existing theories?\n",
    "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
    "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting & Processing\n",
    "- Using RecursiveCharacterTextSplitter in Langchain \n",
    "- Add Source at the end of each chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 20,\n",
    ")\n",
    "\n",
    "texts1 = text_splitter.split_text(text1_content)\n",
    "texts2 = text_splitter.split_text(text2_content)\n",
    "\n",
    "for i in range(len(texts1)):\n",
    "    texts1[i] = texts1[i] + '\\n' + 'Come from: #url1 \\n'\n",
    "    \n",
    "for i in range(len(texts2)):\n",
    "    texts2[i] = texts2[i] + '\\n' + 'Come from: #url2 \\n'\n",
    "\n",
    "texts = texts1 + texts2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with Langchain\n",
    "- Test on RTX3090 using 43.535649061203s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "\n",
    "openai_llm = OpenAI(\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = \"\"\"Write a summary of the following text:\n",
    "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
    "{text}\n",
    "\n",
    "Summary: \n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    template = summarize_template,\n",
    "    input_variables = [\"text\"]\n",
    ")\n",
    "\n",
    "combine_template = \"\"\"Write a summary in detail of the following text:\n",
    "Each bullet point should start with a title in the format of [title], and end with its source in the format of [url1, url2 ...]\n",
    "Not all of the text are relevant, you should not summarize the irrelevant text.\n",
    "\n",
    "Example:\n",
    "[Make Decision] When you get the error message, you should make a decision. [url3, url6]\n",
    "\n",
    "text:\n",
    "<{text}>\n",
    "\n",
    "Summary (in detail):\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template = combine_template,\n",
    "    input_variables = [\"text\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm = openai_llm,\n",
    "    chain_type = \"map_reduce\",\n",
    "    map_prompt = summarize_prompt,\n",
    "    combine_prompt = combine_prompt,\n",
    "    combine_document_variable_name = \"text\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "GPT-3 vs. BERT: Comparing the Two Most Popular Language Models\n",
      "Natural language processing (NLP) has come a long way over the past few years. With the development of powerful new models such as GPT-3 and BERT, it's now possible to create sophisticated applications that can understand and interact with human language.\n",
      "However, what went viral as a disruptive chatbot with ChatGPT, suddenly became a contest of language models to power AI content. So, we decided to oppose GPT-3 vs. BERT to understand their differences and similarities, explore their capabilities, and discuss some of the tools that use them.\n",
      "GPT-3 (Generative Pre-trained Transformer 3) is an autoregressive language model developed by OpenAI. It was trained on a dataset of 45TB of text data from sources such as Wikipedia, books, and webpages. The model is capable of generating human-like text when given a prompt. It can also be used for tasks such as question answering, summarization, translation, and more.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Several AI content writing tools currently use GPT-3, such as:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is another popular language model developed by Google AI. Unlike GPT-3, BERT is a bidirectional transformer model, which considers both left and right context when making predictions. This makes it better suited for sentiment analysis or natural language understanding (NLU) tasks.\n",
      "BERT serves as the base for a number of services, like:\n",
      "The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial.\n",
      "Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT).\n",
      "Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT:\n",
      "They use the Transformer architecture to learn context from textual-based datasets using attention mechanisms.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "They are unsupervised learning models (they don’t require labeled data for training).\n",
      "They can perform various NLP tasks such as question answering, summarization, or translation with varying degrees of accuracy, depending on the task.\n",
      "Both GPT-3 and BERT have been shown to perform well on various NLP tasks, including question answering, summarization, or translation, with varying degrees of accuracy depending on the task at hand.\n",
      "However, due to its larger training dataset size, GPT-3 tends to outperform its predecessor in certain tasks, such as summarization or translation, where having access to more data can be beneficial.\n",
      "On other tasks, such as sentiment analysis or NLU, BERT tends to do better due to its bidirectional nature, which allows it to take into account both left and right context when making predictions. In contrast, GPT -3 only considers left context when predicting words or phrases in a sentence.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "The bottom line is that GPT-3 and BERT have proven themselves valuable tools for performing various NLP tasks with varying degrees of accuracy. However, due to their differences in architecture and training dataset size, each model is better suited for certain tasks than others.\n",
      "For example, GPT-3 is better suited for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. Ultimately, the choice between the two models will depend on your specific needs and which task you are looking to accomplish.\n",
      "Come from: #url1 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Stack Exchange network consists of 181 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.\n",
      "                                        Learn more about Stack Overflow the company, and our products.\n",
      "Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. It only takes a minute to sign up.\n",
      "In the BERT paper, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.\n",
      "In the GPT paper, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.\n",
      "I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "Q1. GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?\n",
      "Q2. Huggingface Gpt2Model contains forward() method. I guess, feeding single data instance to this method is like doing one shot learning?\n",
      "Q4. Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?\n",
      "To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern-\n",
      "Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:\n",
      "Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.\n",
      "The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary of the following text:\n",
      "You must include what the text if from in the end of the summary, for example, [from url1] or [from url2]\n",
      "By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
      "Not the answer you're looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
      "YA novel about a girl in dystopian society who gets surgically altered to be a double for a dead girl\n",
      "Is lying in an application for a job at a private company, signed under penalty of perjury, prosecutable as perjury?\n",
      "DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\". Something new or based on existing theories?\n",
      "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
      "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n",
      "Come from: #url2 \n",
      "\n",
      "\n",
      "Summary: \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a summary in detail of the following text:\n",
      "Each bullet point should start with a title in the format of [title], and end with its source in the format of [url1, url2 ...]\n",
      "Not all of the text are relevant, you should not summarize the irrelevant text.\n",
      "\n",
      "Example:\n",
      "[Make Decision] When you get the error message, you should make a decision. [url3, url6]\n",
      "\n",
      "text:\n",
      "<This text compares two of the most popular language models in natural language processing (NLP): GPT-3 and BERT. GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data from sources such as Wikipedia, books, and webpages. It is capable of generating human-like text when given a prompt and can be used for tasks such as question answering, summarization, translation, and more. BERT is also a powerful language model, used for tasks such as sentiment analysis and question answering. [from url1]\n",
      "\n",
      "GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model, while BERT is a bidirectional transformer model which considers both left and right context when making predictions. This makes BERT better suited for tasks such as sentiment analysis or natural language understanding (NLU). BERT serves as the base for a number of services. [from url1]\n",
      "\n",
      "GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets. The main difference between them lies in their training datasets, with GPT-3 having access to 45TB of data and BERT having access to 3TB of data. GPT-3 is also significantly larger than BERT due to its much more extensive training dataset size. Despite their differences, they share some similarities such as using the Transformer architecture and attention mechanisms. [from url1]\n",
      "\n",
      "GPT-3 and BERT are unsupervised learning models that can be used for various NLP tasks such as question answering, summarization, or translation. Depending on the task, they can have varying degrees of accuracy. GPT-3 tends to outperform BERT in tasks that require more data, such as summarization or translation, while BERT is better suited for tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]\n",
      "\n",
      "GPT-3 and BERT are both powerful tools for Natural Language Processing (NLP) tasks. However, due to their different architectures and training dataset sizes, each model is better suited for different tasks. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. Ultimately, the choice between the two models depends on the specific needs of the user and the task they are trying to accomplish. [from url1]\n",
      "\n",
      "Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. In the BERT paper, it is learnt that BERT is an encoder-only model, involving only transformer encoder blocks. In the GPT paper, it is learnt that GPT is a decoder-only model, involving only transformer decoder blocks. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]\n",
      "\n",
      "This text discusses the differences between GPT-2 and BERT, two models used for natural language processing. GPT-2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. Apart from technical differences in hyper-parameters, there are no other conceptual differences between the two models. [from url2]\n",
      "\n",
      "BERT and other masked language models can be used for zero- or few-shot learning through a method called PET (Pattern-Exploiting Training). This uses templates to exploit the language modeling abilities of BERT, such as for sentiment analysis. Working with GPT-2 is not as straightforward as with BERT, as the forward method returns hidden states that can be used as contextual embeddings. GPT-2 is usually used by sampling from the model, which requires a prompt and can be done through tutorials such as a blog post by Huggingface. [from url2]\n",
      "\n",
      "This text discusses two topics: a YA novel about a girl in a dystopian society who gets surgically altered to be a double for a dead girl, and whether lying in an application for a job at a private company, signed under penalty of perjury, is prosecutable as perjury. It also mentions DeSantis' \"US Constitution’s 'leverage points'... to exercise the 'true scope' of presidential power\" and the Stack Exchange's Cookie Policy. [from url2]>\n",
      "\n",
      "Summary (in detail):\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "43.535649061203\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "summary = map_reduce_chain.run(docs)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with Guidance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using {{#each}} command\n",
    "- Test on RTX3090 using 97.12837052345276s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "\n",
    "guidance.llms.OpenAI.cache.clear()\n",
    "guidance.llm = guidance.llms.OpenAI(\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = []\n",
    "def add_below(text):\n",
    "    summary_list.append(text)\n",
    "\n",
    "summarization_template = \"\"\"\n",
    "Write a concise summary on the content. \n",
    "You must include the source of text in the end of the summary, \n",
    "for example, [from url1] or [from url2]\n",
    "\n",
    "{{~#each text_list}}\n",
    "{{#block hidden=True}}\n",
    "<Content: {{this}}>\n",
    "Summary: {{add_below (gen 'summary' temperature=0)}}\n",
    "{{/block}}\n",
    "{{/each}}\n",
    "\"\"\"\n",
    "\n",
    "summary_program = guidance(\n",
    "    template = summarization_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-07652beb-359b-4776-bbc1-3fa053a9d5c9\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-07652beb-359b-4776-bbc1-3fa053a9d5c9\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'> \n",
       "Write a concise summary on the content. \n",
       "You must include the source of text in the end of the summary, \n",
       "for example, [from url1] or [from url2]<span style='opacity: 1.0; display: inline; background-color: rgba(165, 165, 165, 0.1);' title='{{~#each text_list}}\n",
       "{{#block hidden=True}}\n",
       "&lt;Content: {{this}}&gt;\n",
       "Summary: {{add_below (gen &#x27;summary&#x27; temperature=0)}}\n",
       "{{/block}}\n",
       "{{/each}}'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</span>\n",
       " </pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"07652beb-359b-4776-bbc1-3fa053a9d5c9\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.12837052345276\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "executed_program = summary_program(text_list = texts, add_below = add_below)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' GPT-3 and BERT are two of the most popular language models used in natural language processing (NLP). GPT-3 is an autoregressive language model developed by OpenAI, trained on 45TB of text data from sources such as Wikipedia, books, and webpages. It can generate human-like text when given a prompt and can be used for tasks such as question answering, summarization, translation, and more. BERT is a transformer-based language model developed by Google, trained on a large corpus of text data. It can be used for tasks such as sentiment analysis, question answering, and text classification. This article compares the two models and discusses the tools that use them. [from url1]',\n",
       " ' GPT-3 and BERT are two popular AI content writing tools. GPT-3 is an autoregressive model while BERT is bidirectional, taking into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or natural language understanding. BERT serves as the base for a number of services. [from url1]',\n",
       " ' GPT-3 and BERT are two large language models that use the Transformer architecture to learn context from textual-based datasets. GPT-3 has 1.5 billion parameters and was trained on 45TB of data, while BERT has 340 million parameters and was trained on 3TB of data. GPT-3 is significantly larger than BERT due to its much more extensive training dataset size. Despite their differences, they both use attention mechanisms to learn context. [from url1]',\n",
       " ' GPT-3 and BERT are unsupervised learning models that can perform various NLP tasks such as question answering, summarization, or translation. GPT-3 tends to outperform BERT in tasks such as summarization or translation due to its larger training dataset size, while BERT tends to do better in tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]',\n",
       " ' GPT-3 and BERT are both powerful tools for performing NLP tasks, but each model is better suited for different tasks. GPT-3 is better for summarization and translation, while BERT is better for sentiment analysis and NLU. The choice between the two models depends on the specific needs of the task. [from url1]',\n",
       " ' Stack Exchange network consists of 181 Q&A communities, including Stack Overflow, the largest online community for developers. Data Science Stack Exchange is a question and answer site for Data science professionals, Machine Learning specialists, and those interested in learning more about the field. The BERT paper explains that BERT is an encoder-only model, involving only transformer encoder blocks, while the GPT paper explains that GPT is a decoder-only model, involving only transformer decoder blocks. The difference between encoder and decoder blocks is that GPT Decoder looks only at previously generated tokens and learns from them, while BERT Encoder gives attention to tokens on both sides. [from url2]',\n",
       " ' GPT2 and BERT are two different models with different architectures and objectives. GPT2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively, while BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. There are only technical differences in hyper-parameters, but no other conceptual differences. [from url2]',\n",
       " ' BERT can be used for zero- or few-shot learning through PET (Pattern-Exploiting Training) which uses language modeling abilities of BERT via templates. GPT-2 is usually used by sampling from the model, providing a prompt and hoping the model will continue in a reasonable way. Tutorials on how to generate from GPT-2 models can be found online. [from url2]',\n",
       " ' This content discusses two questions: one about a YA novel about a girl in a dystopian society and the other about lying in an application for a job at a private company. It also mentions DeSantis\\' \"US Constitution’s \\'leverage points\\'... to exercise the \\'true scope\\' of presidential power\" and the use of cookies on Stack Exchange. [from url2]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using {{#geneach}} command\n",
    "- Test on RTX3090 using 79.16032338142395s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "\n",
    "guidance.llms.OpenAI.cache.clear()\n",
    "guidance.llm = guidance.llms.OpenAI(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = []\n",
    "def add_below(text):\n",
    "    summary_list.append(text)\n",
    "\n",
    "summarization_template = \"\"\"\n",
    "{{#system~}}\n",
    "Write a concise summary on the following text. You must include the source of text in the end of the summary, for example, [from url1] or [from url2]\n",
    "{{~/system}}\n",
    "\n",
    "{{~#geneach 'summaries' stop=False}}\n",
    "{{#user~}}\n",
    "{{set 'this.usr_text' (await 'usr_text')}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{add_below (gen 'this.summary' temperature=0 hidden=True)}}\n",
    "{{~/assistant}}\n",
    "{{~/geneach}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "summary_program = guidance(\n",
    "    template = summarization_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-607381cc-42bf-43c4-9316-615291d10eda\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-607381cc-42bf-43c4-9316-615291d10eda\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Write a concise summary on the following text. You must include the source of text in the end of the summary, for example, [from url1] or [from url2]</div></div><span style='opacity: 1.0; display: inline; background-color: rgba(165, 165, 165, 0.1);' title='{{~#geneach &#x27;summaries&#x27; stop=False}}\n",
       "{{#user~}}\n",
       "{{set &#x27;this.usr_text&#x27; (await &#x27;usr_text&#x27;)}}\n",
       "{{~/user}}\n",
       "\n",
       "{{#assistant~}}\n",
       "{{add_below (gen &#x27;this.summary&#x27; temperature=0 hidden=True)}}\n",
       "{{~/assistant}}\n",
       "{{~/geneach}}'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='display: inline;' title='{{set &#x27;this.usr_text&#x27; (await &#x27;usr_text&#x27;)}}'>By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct.\n",
       "Not the answer you&#x27;re looking for? Browse other questions tagged machine-learningnlpberttransformergpt or ask your own question.\n",
       "YA novel about a girl in dystopian society who gets surgically altered to be a double for a dead girl\n",
       "Is lying in an application for a job at a private company, signed under penalty of perjury, prosecutable as perjury?\n",
       "DeSantis&#x27; &quot;US Constitution’s &#x27;leverage points&#x27;... to exercise the &#x27;true scope&#x27; of presidential power&quot;. Something new or based on existing theories?\n",
       "Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2023.5.26.43462\n",
       "By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n",
       "Come from: #url2 \n",
       "</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{add_below (gen &#x27;this.summary&#x27; temperature=0 hidden=True)}}'></span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='display: inline;' title='{{set &#x27;summaries[-1].usr_text&#x27; (await &#x27;usr_text&#x27;)}}'></span><span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{set &#x27;summaries[-1].usr_text&#x27; (await &#x27;usr_text&#x27;)}}</span></div></div><span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{#assistant~}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{add_below (gen &#x27;summaries[-1].summary&#x27; temperature=0 hidden=True)}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{~/assistant}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{~#geneach &#x27;summaries&#x27; stop=False}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{#user~}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{set &#x27;this.usr_text&#x27; (await &#x27;usr_text&#x27;)}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{~/user}}</span>\n",
       "\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{#assistant~}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{add_below (gen &#x27;this.summary&#x27; temperature=0 hidden=True)}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{~/assistant}}</span>\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{~/geneach}}</span></span>\n",
       " </pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"607381cc-42bf-43c4-9316-615291d10eda\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.16032338142395\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for text in texts:\n",
    "    summary_program(usr_text = text, add_below = add_below)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The article discusses the differences and similarities between two popular language models, GPT-3 and BERT, which have revolutionized natural language processing (NLP). GPT-3 is an autoregressive language model developed by OpenAI, trained on a massive dataset of text data from various sources. It can generate human-like text and perform tasks such as question answering, summarization, and translation. The article compares the capabilities of GPT-3 and BERT and discusses some of the tools that use them. [from #url1]',\n",
       " 'The article discusses the differences between GPT-3 and BERT, two popular language models used in AI content writing tools. While GPT-3 is an autoregressive model that only considers the left context when making predictions, BERT is a bidirectional transformer model that takes into account both left and right context, making it better suited for tasks such as sentiment analysis or natural language understanding. [from #url1]',\n",
       " \"GPT-3 and BERT are two natural language processing models that differ in their training datasets and size. GPT-3 was trained on a larger dataset of 45TB compared to BERT's 3TB, giving it an advantage in tasks that require access to more data. GPT-3 is also significantly larger than BERT due to its extensive training dataset. However, both models use the Transformer architecture and attention mechanisms to learn context from textual-based datasets. [from url1]\",\n",
       " 'GPT-3 and BERT are unsupervised learning models that can perform various NLP tasks without requiring labeled data for training. Both models have shown good performance on tasks such as question answering, summarization, and translation, with varying degrees of accuracy depending on the task. GPT-3 tends to outperform BERT in tasks such as summarization or translation due to its larger training dataset size, while BERT performs better in tasks such as sentiment analysis or NLU due to its bidirectional nature. [from url1]',\n",
       " 'GPT-3 and BERT are both useful tools for natural language processing (NLP) tasks, but their differences in architecture and training dataset size make them better suited for certain tasks than others. GPT-3 is better for summarization or translation, while BERT is more beneficial for sentiment analysis or NLU. The choice between the two models will depend on specific needs and the task at hand. [from #url1]',\n",
       " 'Data Science Stack Exchange is a Q&A site for data science professionals and machine learning specialists. The site provides a platform for users to ask and answer questions related to the field. The article discusses the difference between BERT and GPT models, where BERT is an encoder-only model, and GPT is a decoder-only model. The article also highlights the difference between encoder and decoder blocks in these models. [from url2]',\n",
       " 'The article discusses the differences between GPT and BERT models. BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively, while GPT-2 is a decoder-only model trained using the left-to-right language objective and operates autoregressively. The article also addresses questions about building short learning models with encoder-only architecture and the one-shot learning capabilities of the GPT2Model.',\n",
       " \"The PET (Pattern-Exploiting Training) method uses BERT's language modeling abilities via templates for zero- or few-shot learning. On the other hand, working with the GPT-2 model is not as straightforward as with BERT, and the usual way of using GPT-2 is sampling from the model. [from url2]\",\n",
       " \"The text is a disclaimer and terms of service statement from Stack Exchange, a question and answer website. It includes a reminder to users that by posting on the site, they agree to the terms of service and privacy policy, as well as a notice that lying on a job application can be considered perjury. The statement also includes information about the site's design and cookie policy. [from url2]\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
