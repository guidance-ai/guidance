# CI Tests which run on Linux machines

# These access secrets, so should only be run on local branches.

# Ideally, the CI tests would be a single workflow, but several issues
# (especially varied OS support) mean that it is hard to keep a single
# workflow green.

name: CI Tests - Linux
permissions:
  contents: read

on:
  workflow_dispatch:
    inputs:
      commit_id:
        description: 'Branch or Commit ID (optional)'
        required: false
        type: string
  schedule:
    # * is a special character in YAML so we quote this string
    # Run at 09:30 UTC every day
    - cron:  '30 09 * * *'


jobs:
  cpu_small:
    strategy:
      fail-fast: false # Don't cancel all on first failure
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        model:
          - "transformers_gpt2_cpu"
          - "llamacpp_phi3_mini_4k_instruct_cpu"
          - "llamacpp_gemma2_9b_cpu"
    uses: ./.github/workflows/call_cpu_tests.yml
    with:
      os: Large_Linux
      python-version: ${{ matrix.python-version }}
      model: ${{ matrix.model }}
      codeCovPython: ${{ vars.CODECOV_PYTHON }}
    secrets:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  cpu_big:
    strategy:
      fail-fast: false # Don't cancel all on first failure
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        model:
          - "llamacpp_llama2_7b_cpu"
          - "transformers_llama3_8b_cpu"
          - "transformers_phi4_mini_cpu"
    uses: ./.github/workflows/call_cpu_tests.yml
    with:
      os: Large_Linux
      python-version: ${{ matrix.python-version }}
      model: ${{ matrix.model }}
      codeCovPython: ${{ vars.CODECOV_PYTHON }}
    secrets:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  gpu_tests:
    strategy:
      fail-fast: false # Don't cancel all on first failure
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        model:
          - "transformers_gpt2_gpu"
          - "transformers_gemma2_9b_gpu"
          - "llamacpp_llama2_7b_gpu"
          - "transformers_gemma2_9b_cpu"  # CUDA is required for this model
          - "transformers_phi4_mini_gpu"
          - "onnxruntime_phi4_mini_instruct"
    uses: ./.github/workflows/call_gpu_tests.yml
    with:
      os: "gpu-runner"
      python-version: ${{ matrix.python-version }}
      model: ${{ matrix.model }}
      codeCovPython: ${{ vars.CODECOV_PYTHON }}
    secrets:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
